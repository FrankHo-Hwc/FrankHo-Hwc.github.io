<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WelCome!</title>
  
  <subtitle>[object Object]</subtitle>
  <link href="https://frankho-hwc.github.io/atom.xml" rel="self"/>
  
  <link href="https://frankho-hwc.github.io/"/>
  <updated>2023-03-09T08:50:01.311Z</updated>
  <id>https://frankho-hwc.github.io/</id>
  
  <author>
    <name>Frank Ho</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>c++八股文</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/c-%E5%85%AB%E8%82%A1%E6%96%87/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/c-%E5%85%AB%E8%82%A1%E6%96%87/</id>
    <published>2023-03-09T08:50:01.000Z</published>
    <updated>2023-03-09T08:50:01.311Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>GAN讲解</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/GAN%E8%AE%B2%E8%A7%A3/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/GAN%E8%AE%B2%E8%A7%A3/</id>
    <published>2023-03-09T08:49:11.000Z</published>
    <updated>2023-03-09T08:49:11.230Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>resnet讲解</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/</id>
    <published>2023-03-09T08:47:51.000Z</published>
    <updated>2023-03-09T08:47:51.502Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Gram矩阵</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/Gram%E7%9F%A9%E9%98%B5/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/Gram%E7%9F%A9%E9%98%B5/</id>
    <published>2023-03-09T08:47:04.000Z</published>
    <updated>2023-03-09T08:47:04.864Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>transformer讲解</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/transformer%E8%AE%B2%E8%A7%A3/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/transformer%E8%AE%B2%E8%A7%A3/</id>
    <published>2023-03-09T08:46:48.000Z</published>
    <updated>2023-03-09T08:46:48.027Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Normalization串讲</title>
    <link href="https://frankho-hwc.github.io/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/"/>
    <id>https://frankho-hwc.github.io/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/</id>
    <published>2023-03-08T13:23:40.000Z</published>
    <updated>2023-03-09T11:26:27.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Normalization的目的就是使数据分布服从均值为0，方差为1的标准正态分布(高斯分布).其目的在于使神经元输入的数据是独立同分布的，这样可以使得网络更快收敛，而且又不会出现梯度消失的问题。</p><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h2><p>在深度学习中，网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：<strong>“Internal Covariate Shift”</strong>。</p><p>出现ICS问题，就会导致每个神经元的输入数据不再是“独立同分布”了，则会导致</p><ol><li>上层参数需要不断适应新的输入数据分布，降低学习速度。</li><li>下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</li><li>每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li></ol><p>所以为了解决这个问题，就提出了批量归一化</p><p>具体做法如下</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/webp.webp" alt="img"></p><p>为什么最后规范化之后还要尺度变换和平移呢，这个操作是一个归一化的反操作，给予两个参数$\gamma$和$\beta$让神经网络自己去学习，让它自己琢磨normalization到底有没有起到优化作用。</p><h2 id="Batch-Normalization-后的效果"><a href="#Batch-Normalization-后的效果" class="headerlink" title="Batch Normalization 后的效果"></a>Batch Normalization 后的效果</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-95f654fdf99999db3fa7dab0bbfbc358_720w.webp" alt="img"></p><p>输入数据被归一到高斯分布区间，激活函数的敏感性更强了</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-b31f7d863179f5f0b93d40c4fabbc31a_720w.webp" alt="img"></p><p>经过激活函数后的数据更加平滑，有利于之后的训练</p><h2 id="BatchNormalizaiton-图解"><a href="#BatchNormalizaiton-图解" class="headerlink" title="BatchNormalizaiton 图解"></a>BatchNormalizaiton 图解</h2><p><strong>1维</strong></p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/62ae8f9dadb346308041b7ad909be734-16783450068039.png" alt="img"></p><p><strong>2维</strong></p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/183a59ee09834831851930e0a580abcd.png" alt="img"></p><p><strong>3维</strong></p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/0bff3dea63de40549f1e5abbc2f8cc8f.png" alt="img"></p><h2 id="加入BN后的效果"><a href="#加入BN后的效果" class="headerlink" title="加入BN后的效果"></a>加入BN后的效果</h2><ol><li>加快网络收敛速度</li><li>缓解梯度爆炸和防止梯度消失，由上图<a href="##Batch Normalization 后的效果">Batch Normalization后的效果</a></li><li>防止过拟合</li></ol><h1 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h1><h2 id="详解-1"><a href="#详解-1" class="headerlink" title="详解"></a>详解</h2><p>如果是遇到样本序列长度不同的时候，如RNN，transformer等，无法使用BN来进行归一化，如下图这种情况，如果使用BN的话，会出现batchsize过小的问题。同时样本数过少时，BN也不能发挥出它应该有的效果。</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/3ca9141cc2224748993520f6db520bf4.png" alt="img"></p><p>具体公式如下</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/image-20230309192458979.png" alt="image-20230309192458979"></p><p>LN中同层神经元的输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。</p><p>对于特征图</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/7000.png" alt="img"></p><p> ，LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。其均值和标准差公式为：</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/7000-16783611681623.png" alt="img"></p><h2 id="与BN的对比"><a href="#与BN的对比" class="headerlink" title="与BN的对比"></a>与BN的对比</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-c039daa05cd9d5c3936c4513422690b0_720w.jpeg" alt="img"></p><p>如图所示，左边是LayerNormalizaiton，而右边是BatichNormalization。BN是按照batch来切的，batch中每一个样本的同一个维度来进行normalization。而LN则是对同一个样本的不同通道进行normalization。</p><h2 id="LN的效果"><a href="#LN的效果" class="headerlink" title="LN的效果"></a>LN的效果</h2><p>同样地，LN也能够很好地缓解ICS问题。</p><h1 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h1><h2 id="详解-2"><a href="#详解-2" class="headerlink" title="详解"></a>详解</h2><h2 id="BN-LN-IN三者对比"><a href="#BN-LN-IN三者对比" class="headerlink" title="BN,LN,IN三者对比"></a>BN,LN,IN三者对比</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-94c40b6f6f41e45f5d254906d70c10ee_720w.webp" alt="img"></p><p>如上图所示，最左边是LN,中间是BN，最右边是IN。</p><h1 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h1><h2 id="详解-3"><a href="#详解-3" class="headerlink" title="详解"></a>详解</h2><p>GN介于LN和IN之间，其首先将channel分为许多组（group），对每一组做归一化，及先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]</p><h2 id="BN-LN-IN-GN四者对比"><a href="#BN-LN-IN-GN四者对比" class="headerlink" title="BN,LN,IN,GN四者对比"></a>BN,LN,IN,GN四者对比</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-fad3333df9a87c1c4f1db4b20557da6f_720w.webp" alt="img"></p><p>BatchNorm：batch方向做归一化，算N<em>H</em>W的均值<br>LayerNorm：channel方向做归一化，算C<em>H</em>W的均值<br>InstanceNorm：一个channel内做归一化，算H<em>W的均值<br>GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)</em>H*W的均值</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/480425962">Internal Covariate Shift问题 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/24810318">什么是批标准化 (Batch Normalization) - 知乎 (zhihu.com)</a></p><p><a href="https://www.jianshu.com/p/a78470f521dd">内部协变量偏移(Internal Covariate Shift)和批归一化(Batch Normalization) - 简书 (jianshu.com)</a></p><p><a href="https://blog.csdn.net/Mike_honor/article/details/125915321">(38条消息) 标准化（Normalization）知识点总结_normalization操作汇总_CV技术指南的博客-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/54530247">模型优化之Layer Normalization - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/492803886">Transformer中的归一化(五)：Layer Norm的原理和实现 &amp; 为什么Transformer要用LayerNorm - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/56542480">模型优化之Instance Normalization - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/35005794">全面解读Group Normalization-（吴育昕-何恺明 ） - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/duanshao/article/details/80055887">(38条消息) 组归一化（Group Normalization）的解释_技术修行的博客-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/74476637">深度学习之17——归一化(BN+LN+IN+GN) - 知乎 (zhihu.com)</a></p><p><a href="https://cloud.tencent.com/developer/article/1526775">https://cloud.tencent.com/developer/article/1526775</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;Normalization的目的就是使数据分布服从均值为0，方差为1的标准正态分布(高斯分布).其目的在于使神经元输入的数据是独立同分布的，</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>双线性插值卷积与反卷积</title>
    <link href="https://frankho-hwc.github.io/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/"/>
    <id>https://frankho-hwc.github.io/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/</id>
    <published>2023-03-08T08:06:31.000Z</published>
    <updated>2023-03-08T13:36:22.371Z</updated>
    
    <content type="html"><![CDATA[<h1 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h1><p>在介绍反卷积之前，先介绍一下卷积</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>计算机视觉里的卷积操作本质上就是数学分析里的卷积详情见下面链接</p><p>具体操作为</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308191953527.png" alt="image-20230308191953527"></p><p>用矩阵法来理解为</p><p>假设输入图像为 4*4，元素矩阵为</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192113451.png" alt="image-20230308192113451"></p><p>卷积核大小为3*3</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192131123.png" alt="image-20230308192131123"></p><p>步长stride = 1， 填充padding = 0，按照卷积计算公式$output = \frac{i + 2p -k}{s} + 1$,则输出矩阵为2*2</p><p>将输入图像flatten成1维向量</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192245137.png" alt="image-20230308192245137"></p><p>同样地，将输出Flatten成1维向量</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192325607.png" alt="image-20230308192325607"></p><p>用矩阵运算来描述</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192336855.png" alt="image-20230308192336855"></p><p>推导可知稀疏矩阵$C$：</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192351512.png" alt="image-20230308192351512"></p><h2 id="反卷积-1"><a href="#反卷积-1" class="headerlink" title="反卷积"></a>反卷积</h2><p>反卷积，顾名思义就是卷积的反操作，但是对于同一个卷积核（因非其稀疏矩阵不是正交矩阵），<strong>结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状</strong></p><p>以矩阵运算来描述</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-556e4849c9bf764fdbfb5ce84f0a204d_720w.webp" alt="img"></p><h2 id="棋盘效应"><a href="#棋盘效应" class="headerlink" title="棋盘效应"></a>棋盘效应</h2><p>在使用转置卷积时观察到一个棘手的现象（尤其是深色部分常出现）就是”<a href="https://www.zhihu.com/search?q=棋盘格子状伪影&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A1682194600}">棋盘格子状伪影</a>“，被命名为棋盘效应（Checkboard artifacts）。</p><p>棋盘效应是由于转置卷积的“不均匀重叠”（Uneven overlap）的结果。使图像中某个部位的颜色比其他部位更深。尤其是当卷积核（Kernel）的大小不能被步长（Stride）整除时，<a href="https://www.zhihu.com/search?q=反卷积&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A1682194600}">反卷积</a>就会不均匀重叠。虽然原则上网络可以通过训练调整权重来避免这种情况，但在实践中神经网络很难完全避免这种不均匀重叠。</p><p>​    在（a）中，步长为1，卷积核为$2*2$。如红色部分所展示，输入第一个像素映射到输出上第一个和第二个像素。而正如绿色部分，输入的第二个像素映射到输出上的第二个和第三个像素。则输出上的第二个像素从输入上的第一个和第二个像素接收信息。总而言之，输出中间部分的像素从输入中接收的信息存在重叠区域。在示例（b）中的卷积核大小增加到3时，输出所接收到的大多数信息的中心部分将收缩。但这并不是最大的问题，因为重叠仍然是均匀的。</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-9a93cbac03830084b90574f017b6038b_720w.webp" alt="img"></p><p>如果将步幅改为2，在卷积核大小为2的示例中，输出上的所有像素从输入中接收相同数量的信息。由下图（a）可见，此时描以转置卷积的重叠。若将卷积核大小改为4（下图（b）），则均匀重叠区域将收缩，与此同时因为重叠是均匀的，故仍然为有效输出。但如果将卷积核大小改为3，步长为2（下图（c）），以及将卷积核大小改为5，步长为2（下图（d）），问题就出现了，对于这两种情况输出上的每个像素接收的信息量与相邻像素不同。在输出上找不到连续且均匀重叠区域。</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-6288d1734ad00c718fc814e4c7bbc985_720w.webp" alt="img"></p><p> 在二维情况下棋盘效应更为严重，下图直观地展示了在二维空间内的棋盘效应。</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-de1bb8f86193666e3b0a1539d273ab32_720w.webp" alt="img"></p><h3 id="如何避免"><a href="#如何避免" class="headerlink" title="如何避免"></a>如何避免</h3><p><strong>采取可以被步长整除的卷积核长度</strong><br>该方法较好地应对了棋盘效应问题，但仍然不够圆满，因为一旦我们的卷积核学习不均匀。</p><p><strong>线性插值</strong></p><h2 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h2><p>在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算<strong>(e.g.:图像的语义分割(segmentation))</strong>，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做<strong>上采样(Upsample)</strong></p><h2 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h2><p>下采样实际上就是缩小图像，主要目的是为了使得图像符合显示区域的大小，生成对应图像的缩略图。比如说在CNN中的池化层或卷积层就是下采样。不过卷积过程导致的图像变小是为了提取特征，而池化下采样是为了降低特征的维度。下采样层有两个作用：<br>一是减少计算量，防止过拟合；<br>二是增大感受野，使得后面的卷积核能够学到更加全局的信息。</p><h1 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h1><h2 id="双线性插值-Bilinear-Interpolation）"><a href="#双线性插值-Bilinear-Interpolation）" class="headerlink" title="双线性插值(Bilinear Interpolation）"></a>双线性插值(Bilinear Interpolation）</h2><p>在讲双线性插值卷积前，先讲单线性插值卷积</p><h3 id="单线性插值"><a href="#单线性插值" class="headerlink" title="单线性插值"></a>单线性插值</h3><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308163812548.png" alt="image-20230308163812548"></p><p>如图所示，已知中P1点和P2点，坐标分别为(x1, y1)、(x2, y2)，要计算 [x1, x2] 区间内某一位置 x 在直线上的y值</p><p>由两点确定一条直线可知</p><script type="math/tex; mode=display">\frac{y-y_1}{x - x_1} = \frac{y_2 - y_1}{x_2 - x_1}</script><p>经过整理可得</p><script type="math/tex; mode=display">y = \frac{x_2 - x}{x_2 - x_1}y_1 - \frac{x - x_1}{x_2 - x_1} y_2</script><p>首先看分子，分子可以看成x与x1和x2的距离作为权重，这也是很好理解的，P点与P1、P2点符合线性变化关系，所以P离P1近就更接近P1，反之则更接近P2。</p><p>现在再把公式中的分式看成一个整体，原式可以理解成y1与y2是加权系数，如何理解这个加权，要返回来思考一下，咱们先要明确一下根本的目的：咱们现在不是在求一个公式，而是在图像中根据2个点的像素值求未知点的像素值。这样一个公式是不满足咱们写代码的要求的。<br>现在根据实际的目的理解，就很好理解这个加权了，y1与y2分别代表原图像中的像素值，上面的公式可以写成如下形式：</p><script type="math/tex; mode=display">f(P) =\frac{x_2 - x}{x_2 - x_1}f(P_1) - \frac{x - x_1}{x_2 - x_1} f(P_2)</script><p>其中，$P$,$P_1$,$P_2$分别代表了被插值的像素和插值两边的像素</p><h3 id="再看双线性插值"><a href="#再看双线性插值" class="headerlink" title="再看双线性插值"></a>再看双线性插值</h3><p>双线性插值是分别在两个方向计算了共3次单线性插值，如图所示，先在x方向求2次单线性插值，获得R1(x, y1)、R2(x, y2)两个临时点，再在y方向计算1次单线性插值得出P(x, y)（实际上调换2次轴的方向先y后x也是一样的结果）。<br><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308164511503.png" alt="image-20230308164511503"></p><p>首先是插值出$R_1$和$R_2$</p><script type="math/tex; mode=display">f(R_1) =\frac{x_2 - x}{x_2 - x_1}f(Q_{11}) - \frac{x - x_1}{x_2 - x_1} f(Q_{21}) \\f(R_2) =\frac{x_2 - x}{x_2 - x_1}f(Q_{12}) - \frac{x - x_1}{x_2 - x_1} f(Q_{22})</script><p>然后再通过$R_1$和$R_2$插值出P</p><script type="math/tex; mode=display">f(P) =\frac{y_2 - y}{y_2 - y_1}f(R_{1}) - \frac{y - y_1}{y_2 - y_1} f(R_{2})</script><p>总结起来就是</p><script type="math/tex; mode=display">f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) + \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x - x_1)(y_2 - y) + \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2 - x_1)(y_2 -y _1) + \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x - x1)(y - y2)</script><h1 id="双线性插值与反卷积的关系"><a href="#双线性插值与反卷积的关系" class="headerlink" title="双线性插值与反卷积的关系"></a>双线性插值与反卷积的关系</h1><p>双线性插值与反卷积都可以用来实现上采样，而双线性插值可以通过反卷积实现。而插值方法不止一种，还有三线性插值等。插值方法的优点是可以避免棋盘效应。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/48501100">反卷积(Transposed Convolution)详细推导 - 知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/22298352">知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/48279880/answer/1682194600">https://www.zhihu.com/question/48279880/answer/1682194600</a></p><p>对上述帖子致以诚挚感谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;反卷积&quot;&gt;&lt;a href=&quot;#反卷积&quot; class=&quot;headerlink&quot; title=&quot;反卷积&quot;&gt;&lt;/a&gt;反卷积&lt;/h1&gt;&lt;p&gt;在介绍反卷积之前，先介绍一下卷积&lt;/p&gt;
&lt;h2 id=&quot;卷积&quot;&gt;&lt;a href=&quot;#卷积&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>名词解释-ablation</title>
    <link href="https://frankho-hwc.github.io/2023/03/08/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-Ablation%20study/"/>
    <id>https://frankho-hwc.github.io/2023/03/08/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-Ablation%20study/</id>
    <published>2023-03-08T07:56:36.000Z</published>
    <updated>2023-03-08T08:53:04.267Z</updated>
    
    <content type="html"><![CDATA[<h1 id="何为Ablation-study"><a href="#何为Ablation-study" class="headerlink" title="何为Ablation study"></a>何为Ablation study</h1><p><strong>Ablation study</strong>，意为消融实验，<strong>通常是指删除模型或算法的某些“功能”，并查看其如何影响性能。</strong></p><p>在论文中一般来说会提出多个创新方法，或者新型结构模块，或注意力模块等。这些东西在一起为模型的性能作出了贡献。然而为了了解每个部分单独能发挥的作用，常常会在论文中提出消融研究。</p><p>例如某论文提出了方法A,B,C。而该论文是基于某个baseline的改进。因此，在消融研究部分，会进行以下实验，baseline ，baseline+A，baseline+B, baseline+C, baseline+A+B+C等实验的各个评价指标有多少，从而得出每个部分所能发挥的作用有多大。</p><p>参考文章<a href="https://zhuanlan.zhihu.com/p/389091953">名词解释 | 论文中的Ablation study - 知乎 (zhihu.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;何为Ablation-study&quot;&gt;&lt;a href=&quot;#何为Ablation-study&quot; class=&quot;headerlink&quot; title=&quot;何为Ablation study&quot;&gt;&lt;/a&gt;何为Ablation study&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Ablati</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>EnLightenGAN</title>
    <link href="https://frankho-hwc.github.io/2023/03/04/EnLightenGAN/"/>
    <id>https://frankho-hwc.github.io/2023/03/04/EnLightenGAN/</id>
    <published>2023-03-04T12:03:15.000Z</published>
    <updated>2023-03-08T13:38:46.753Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EnligtenGAN"><a href="#EnligtenGAN" class="headerlink" title="EnligtenGAN"></a>EnligtenGAN</h1><h1 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h1><p>首先我们可以看到有scripts文件夹，我们运行的程序就在这个里面‘</p><p>通过分析代码我们可以知道实际运行的还是train.py和predict.py</p><p><img src="/2023/03/04/EnLightenGAN/image-20230306215659710.png" alt="image-20230306215659710"></p><p>然后我们可以发现有一个option文件夹，里面就是有附带的一些选项。</p><p>configs里有一个yaml文件，里面配置了enlightenGAN的超参数。</p><p>data文件夹里定义了dataloader，即如何将数据读取进去。</p><p>最关键的是models里的singlemodel和networks这两个py文件，其他文件我觉得应该是一些替代组件，应该是用来做消融实验的？</p><p>此处具体讲解的就是这两个文件</p><p>这个模型使用是一个U-Net架构的Generator和两个Discirminator,具体之后更新</p><h1 id="代码运行及一些问题"><a href="#代码运行及一些问题" class="headerlink" title="代码运行及一些问题"></a>代码运行及一些问题</h1><h2 id="运行步骤"><a href="#运行步骤" class="headerlink" title="运行步骤"></a>运行步骤</h2><h3 id="1-从github上git-clone源码"><a href="#1-从github上git-clone源码" class="headerlink" title="1. 从github上git clone源码"></a>1. 从github上git clone源码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/VITA-Group/EnlightenGAN</span><br></pre></td></tr></table></figure><h3 id="2-创建虚拟环境-也可以使用以前的环境-并安装依赖"><a href="#2-创建虚拟环境-也可以使用以前的环境-并安装依赖" class="headerlink" title="2.创建虚拟环境(也可以使用以前的环境),并安装依赖"></a>2.创建虚拟环境(也可以使用以前的环境),并安装依赖</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate enlighten</span><br><span class="line">pip install -r requirement.txt</span><br></pre></td></tr></table></figure><h3 id="3-创建文件夹-并将vgg预训练模型放入其中"><a href="#3-创建文件夹-并将vgg预训练模型放入其中" class="headerlink" title="3.创建文件夹,并将vgg预训练模型放入其中"></a>3.创建文件夹,并将vgg预训练模型放入其中</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir model</span><br></pre></td></tr></table></figure><p>模型地址：<a href="https://drive.google.com/file/d/1IfCeihmPqGWJ0KHmH-mTMi_pn3z3Zo-P/view?usp=sharing">https://drive.google.com/file/d/1IfCeihmPqGWJ0KHmH-mTMi_pn3z3Zo-P/view?usp=sharing</a></p><p>本人提供一个百度网盘 链接：<a href="https://pan.baidu.com/s/1qX97-7H3HCwllLwiBUo_Zg?pwd=ktnj">https://pan.baidu.com/s/1qX97-7H3HCwllLwiBUo_Zg?pwd=ktnj</a>  code：ktnj </p><h3 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h3><h4 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><p> …/final_dataset/trainA and …/final_dataset/trainB（即final_dataset文件夹与项目文件夹同级的位置），将<a href="https://drive.google.com/drive/folders/1fwqz8-RnTfxgIIkebFG2Ej3jQFsYECh0?usp=sharing">图片</a>下载分别放入</p><p>如无法下载，此处有百度网盘</p><p>链接：<a href="https://pan.baidu.com/s/1MpSKs5HVs6alMjfzQtm3rA?pwd=vgiy">https://pan.baidu.com/s/1MpSKs5HVs6alMjfzQtm3rA?pwd=vgiy</a>  code：vgiy </p><h4 id="可视化-可选"><a href="#可视化-可选" class="headerlink" title="可视化(可选)"></a>可视化(可选)</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python -m visdom.server -port=8097</span><br></pre></td></tr></table></figure><p>访问步骤</p><p>打开浏览器，输入<a href="http://localhost:8097/（可以实时观看图片结果）">http://localhost:8097/（可以实时观看图片结果）</a><br>如果在另一台电脑上跑，输入地址：地址号：端口号<br>如：10.162.34.109:8097</p><h4 id="进行训练"><a href="#进行训练" class="headerlink" title="进行训练"></a>进行训练</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python scripts/script.py --train</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>可能遇到问题</strong></p><p>如果你的集群的卡不是三张，请打开scripts文件夹下的script.py,将第37行 —gpu_ids 修改为你所拥有显卡数量的编号</p><p><img src="/2023/03/04/EnLightenGAN/image-20230304202357977.png" alt="image-20230304202357977"></p><p>我使用的是有两张3090的服务器，所以修改位0,1</p><p>否则将会出现这样的错误</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CustomDatasetDataLoader</span><br><span class="line">dataset [UnalignedDataset] was created</span><br><span class="line">#training images = 1016</span><br><span class="line">single</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;train.py&quot;, line 19, in &lt;module&gt;</span><br><span class="line">    model = create_model(opt)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/models.py&quot;, line 36, in create_model</span><br><span class="line">    model.initialize(opt)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/single_model.py&quot;, line 40, in initialize</span><br><span class="line">    self.vgg = networks.load_vgg16(&quot;./model&quot;, self.gpu_ids)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/networks.py&quot;, line 1051, in load_vgg16</span><br><span class="line">    vgg = torch.nn.DataParallel(vgg, gpu_ids)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 142, in __init__</span><br><span class="line">    _check_balance(self.device_ids)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 23, in _check_balance</span><br><span class="line">    dev_props = _get_devices_properties(device_ids)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 458, in _get_devices_properties</span><br><span class="line">    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 458, in &lt;listcomp&gt;</span><br><span class="line">    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 441, in _get_device_attr</span><br><span class="line">    return get_member(torch.cuda)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 458, in &lt;lambda&gt;</span><br><span class="line">    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 299, in get_device_properties</span><br><span class="line">    raise AssertionError(&quot;Invalid device id&quot;)</span><br><span class="line">AssertionError: Invalid device id</span><br></pre></td></tr></table></figure><p>此外，还要修改一个地方</p><p><img src="/2023/03/04/EnLightenGAN/image-20230304203105671.png" alt="image-20230304203105671"></p><p>否则会报错，是因为YAML 5.1版本后弃用了yaml.load(file)这个用法，因为觉得很不安全，5.1版本之后就修改了需要指定Loader，通过默认加载器（FullLoader）禁止执行任意函数，该load函数也变得更加安全。</p><p>此外还有个报错</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model [SingleGANModel] was created</span><br><span class="line">Setting up a new session...</span><br><span class="line">create web directory ./checkpoints/enlightening/web...</span><br><span class="line">/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.</span><br><span class="line">  warnings.warn(&quot;nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.&quot;)</span><br><span class="line">/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.</span><br><span class="line">  &quot;See the documentation of nn.Upsample for details.&quot;.format(mode))</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;train.py&quot;, line 37, in &lt;module&gt;</span><br><span class="line">    errors = model.get_current_errors(epoch)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/single_model.py&quot;, line 413, in get_current_errors</span><br><span class="line">    D_A = self.loss_D_A.data[0]</span><br><span class="line">IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item&lt;T&gt;()` in C++ to convert a 0-dim tensor to a number</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个就是把single_model.py里所有的data[0]改成.item就行了</p><h3 id="5-评估"><a href="#5-评估" class="headerlink" title="5.评估"></a>5.评估</h3><h4 id="仅测试时："><a href="#仅测试时：" class="headerlink" title="仅测试时："></a>仅测试时：</h4><p>下载pretrained model，放入./checkpoints/enlightening中</p><h4 id="创建文件夹-1"><a href="#创建文件夹-1" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><p>…/test_dataset/testA and …/test_dataset/testB（即test_dataset文件夹与项目文件夹同级的位置），将自己要测试的图片放入testA，在testB中至少存入一张随机图片</p><h4 id="运行测试代码"><a href="#运行测试代码" class="headerlink" title="运行测试代码"></a>运行测试代码</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python scripts/script.py --predict</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;EnligtenGAN&quot;&gt;&lt;a href=&quot;#EnligtenGAN&quot; class=&quot;headerlink&quot; title=&quot;EnligtenGAN&quot;&gt;&lt;/a&gt;EnligtenGAN&lt;/h1&gt;&lt;h1 id=&quot;代码讲解&quot;&gt;&lt;a href=&quot;#代码讲解&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="低亮图像增强" scheme="https://frankho-hwc.github.io/tags/%E4%BD%8E%E4%BA%AE%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>JavaIO机制</title>
    <link href="https://frankho-hwc.github.io/2023/03/02/JavaIO%E6%9C%BA%E5%88%B6/"/>
    <id>https://frankho-hwc.github.io/2023/03/02/JavaIO%E6%9C%BA%E5%88%B6/</id>
    <published>2023-03-02T01:08:36.000Z</published>
    <updated>2023-03-02T02:22:19.778Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Java-I-O流定义"><a href="#一、Java-I-O流定义" class="headerlink" title="一、Java I/O流定义"></a>一、Java I/O流定义</h1><p>在Java中，流是从源读取并写入目标的数据序列。</p><p><strong>输入流</strong>是从源读取数据，<strong>输出流</strong>是将数据写入目标</p><p>下面就具体讲解Java I/O流</p><h1 id="二、Java-字节流"><a href="#二、Java-字节流" class="headerlink" title="二、Java 字节流"></a>二、Java 字节流</h1><h2 id="2-1-输入流"><a href="#2-1-输入流" class="headerlink" title="2.1 输入流"></a>2.1 输入流</h2><h3 id="2-1-1-方法"><a href="#2-1-1-方法" class="headerlink" title="2.1.1 方法"></a>2.1.1 方法</h3><p>InputStream读入的单位是字节，具体方法如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">read() <span class="comment">// 从输入流中读取数据的下一个字节</span></span><br><span class="line">read(<span class="type">byte</span>[]b) <span class="comment">//从输入流中读取一定长度的字节，并以整数的形式返回字节数</span></span><br><span class="line">mark（<span class="type">int</span> readlimit) <span class="comment">// 在输入流当前位置放一个标记，readlimit参数告知此输入流在标记位置失效之前允许读取的字节数</span></span><br><span class="line">reset()  <span class="comment">//将输入指针返回标记处</span></span><br><span class="line">skip(<span class="type">long</span> n) <span class="comment">//跳过n个单位的字节，并返回实际跳过的字节数</span></span><br><span class="line">markSupported() <span class="comment">//是否支持mark</span></span><br><span class="line">close()<span class="comment">//关闭流</span></span><br></pre></td></tr></table></figure><p>但是并不是所有InputStream类都支持上述方法。</p><h3 id="2-1-2-种类"><a href="#2-1-2-种类" class="headerlink" title="2.1.2 种类"></a>2.1.2 种类</h3><p>以下为常见的三个输入流</p><h4 id="FileInputStream"><a href="#FileInputStream" class="headerlink" title="FileInputStream"></a>FileInputStream</h4><p>该流是用于读取文件中信息的输入流</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String args[])</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">byte</span>[] array = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">100</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">InputStream</span> <span class="variable">input</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="string">&quot;input.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">&quot;文件中的可用字节: &quot;</span> + input.available());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//从输入流中读取字节</span></span><br><span class="line">            input.read(array);</span><br><span class="line">            System.out.println(<span class="string">&quot;从文件读取的数据: &quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将字节数组转换为字符串</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(array);</span><br><span class="line">            System.out.println(data);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭输入流</span></span><br><span class="line">            input.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.getStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这是文件中的一行文本。</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文件中的可用字节: 35</span><br><span class="line">从文件读取的数据:</span><br><span class="line">这是文件中的一行文本。</span><br></pre></td></tr></table></figure><h4 id="InputStream"><a href="#InputStream" class="headerlink" title="InputStream"></a>InputStream</h4><p>为了使用InputStream,首先需要引入java.io.InputStream包</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">InputStream</span> <span class="variable">object1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(); <span class="comment">// 创建输入流对象</span></span><br></pre></td></tr></table></figure><p>示例</p><h4 id="BufferedInputStream"><a href="#BufferedInputStream" class="headerlink" title="BufferedInputStream"></a>BufferedInputStream</h4><p>BufferedInputStream 是为I/O流增加缓冲区，提高性能,该输入流有两个注入方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BufferedInputStream(InputStream in) <span class="comment">//默认size为32</span></span><br><span class="line">BufferedInputStream(InputStream in, <span class="type">int</span> size)</span><br></pre></td></tr></table></figure><h2 id="2-2-输出流"><a href="#2-2-输出流" class="headerlink" title="2.2 输出流"></a>2.2 输出流</h2><h3 id="2-2-1-方法"><a href="#2-2-1-方法" class="headerlink" title="2.2.1 方法"></a>2.2.1 方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">write(<span class="type">int</span> b) <span class="comment">//将指定的字节写入输入流</span></span><br><span class="line">write(<span class="type">byte</span>[]b) <span class="comment">//将b个字节从指定的byte数组写入此输入流</span></span><br><span class="line">write(<span class="type">byte</span>[]b,<span class="type">int</span> off, <span class="type">int</span> len) <span class="comment">//将指定byte数组中从偏移量off开始的len个字节写入此输出流</span></span><br><span class="line">flush() <span class="comment">//彻底完成输出并清空缓存区</span></span><br><span class="line">close() <span class="comment">//关闭输出流</span></span><br></pre></td></tr></table></figure><h3 id="2-2-2-种类"><a href="#2-2-2-种类" class="headerlink" title="2.2.2 种类"></a>2.2.2 种类</h3><h4 id="FileOutputStream"><a href="#FileOutputStream" class="headerlink" title="FileOutputStream"></a>FileOutputStream</h4><p>创建方式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//包括布尔型参数</span></span><br><span class="line"><span class="type">FileOutputStream</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(String path, <span class="type">boolean</span> value);</span><br><span class="line"></span><br><span class="line"><span class="comment">//不包括布尔型参数</span></span><br><span class="line"><span class="type">FileOutputStream</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(String path);</span><br></pre></td></tr></table></figure><p>写入文件</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> <span class="string">&quot;这是文件中的一行文本。&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">FileOutputStream</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="string">&quot;output.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">byte</span>[] array = data.getBytes();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将字节写入文件</span></span><br><span class="line">            output.write(array);</span><br><span class="line"></span><br><span class="line">            output.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">            e.getStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="/2023/03/02/JavaIO%E6%9C%BA%E5%88%B6/image-20230302101400910.png" alt="写入文件"></p><h4 id="OutputStream"><a href="#OutputStream" class="headerlink" title="OutputStream"></a>OutputStream</h4><p>与InputStream对应</p><h4 id="BufferedOutputStream"><a href="#BufferedOutputStream" class="headerlink" title="BufferedOutputStream"></a>BufferedOutputStream</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BufferedOutputStream(OutputStream in) <span class="comment">//默认size为32</span></span><br><span class="line">BufferedOutputStream(OutputStream in, <span class="type">int</span> size)</span><br></pre></td></tr></table></figure><h2 id="三、-Java的字符流"><a href="#三、-Java的字符流" class="headerlink" title="三、 Java的字符流"></a>三、 Java的字符流</h2><h3 id="3-1-Reader"><a href="#3-1-Reader" class="headerlink" title="3.1 Reader"></a>3.1 Reader</h3><p>Reader是用于读取字符的字符流，Stream可能会出现读取汉字乱码的现象</p><h3 id="3-1-1分类"><a href="#3-1-1分类" class="headerlink" title="3.1.1分类"></a>3.1.1分类</h3><h4 id="BufferedReader"><a href="#BufferedReader" class="headerlink" title="BufferedReader"></a>BufferedReader</h4><h4 id="FileReader"><a href="#FileReader" class="headerlink" title="FileReader"></a>FileReader</h4><h3 id="3-2Writrer"><a href="#3-2Writrer" class="headerlink" title="3.2Writrer"></a>3.2Writrer</h3><h2 id="3-2-1"><a href="#3-2-1" class="headerlink" title="3.2.1"></a>3.2.1</h2><h4 id="BufferedWriter"><a href="#BufferedWriter" class="headerlink" title="BufferedWriter"></a>BufferedWriter</h4><h4 id="FlieReader"><a href="#FlieReader" class="headerlink" title="FlieReader"></a>FlieReader</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、Java-I-O流定义&quot;&gt;&lt;a href=&quot;#一、Java-I-O流定义&quot; class=&quot;headerlink&quot; title=&quot;一、Java I/O流定义&quot;&gt;&lt;/a&gt;一、Java I/O流定义&lt;/h1&gt;&lt;p&gt;在Java中，流是从源读取并写入目标的数据序列。&lt;/</summary>
      
    
    
    
    
    <category term="java学习" scheme="https://frankho-hwc.github.io/tags/java%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>JVM学习(1)</title>
    <link href="https://frankho-hwc.github.io/2023/02/27/JVM%E5%AD%A6%E4%B9%A0-(1)/"/>
    <id>https://frankho-hwc.github.io/2023/02/27/JVM%E5%AD%A6%E4%B9%A0-(1)/</id>
    <published>2023-02-27T14:12:03.000Z</published>
    <updated>2023-03-02T01:32:38.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是JVM"><a href="#什么是JVM" class="headerlink" title="什么是JVM"></a>什么是JVM</h1><p>JVM(Java Virtual Machine)即java的虚拟机，是用于在实际的计算机上仿真模拟计算机的实现</p><p><img src="/2023/02/27/JVM%E5%AD%A6%E4%B9%A0-(1)/d947f91e44c44c6c80222b49c2dee859-new-image19a36451-d673-486e-9c8e-3c7d8ab66929.png" alt="java虚拟机示意图"></p><p>​    如图所示，这个就是JVM结构图</p><h1 id="Java内存区域"><a href="#Java内存区域" class="headerlink" title="Java内存区域"></a>Java内存区域</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是JVM&quot;&gt;&lt;a href=&quot;#什么是JVM&quot; class=&quot;headerlink&quot; title=&quot;什么是JVM&quot;&gt;&lt;/a&gt;什么是JVM&lt;/h1&gt;&lt;p&gt;JVM(Java Virtual Machine)即java的虚拟机，是用于在实际的计算机上仿真模拟计算机的</summary>
      
    
    
    
    
    <category term="java学习" scheme="https://frankho-hwc.github.io/tags/java%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="工作" scheme="https://frankho-hwc.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>杂言</title>
    <link href="https://frankho-hwc.github.io/2023/02/26/%E6%9D%82%E8%A8%80/"/>
    <id>https://frankho-hwc.github.io/2023/02/26/%E6%9D%82%E8%A8%80/</id>
    <published>2023-02-26T12:44:35.000Z</published>
    <updated>2023-02-27T11:50:57.672Z</updated>
    
    <content type="html"><![CDATA[<p>2022年12月25日，青岛开发区四中，一年一度的全国研究生考试开始了。青岛刺骨的寒风，没有温度的太阳。一场猝不及防的新冠，以及对数学的恐惧，不出意外，考砸了，没书读了。但是个人还是无法接受这个结果，虽然明知是自己自作自受:前三年如果博一把，就不用遭这门罪;如果再对自己狠点，就可以发挥好些了……</p><p>考研这半年来，啥罪都遭了，啥好事都没轮到自己。看着保研的天天笑嘻嘻，看着留学的炫自己的offer，现在出成绩了，又要看着上岸的同辈的通知书了。看着自己的同辈就要进入下一个人生阶段了，再看看自己原地踏步，想想真是五味杂陈。没办法啊，谁叫你技不如人呢？谁叫你明事理太晚了呢？谁叫你脸皮薄不去找老师进实验室呢？明明有高人指点，居然还能落后别人。应该反省反省自己。</p><p>那就反省反省吧。我觉得我这个人，1、做事太拖沓了；2、心态还是出了大问题，还是不能太急躁了；3、碰到事情还是太直来直去了，不知变通；4、抗压能力还是差了很多；5、缺少定力，容易对外界事物影响；6、有时候还是管不住自己，承接上点；7、某些时候又有点自负了。总之个人的缺点还是很多，没上岸也说明老天可能觉得我必须要改完这些才有资格上岸吧。</p><p>那既然没上岸，首要议程就是接下来怎么办，因为要毕业了嘛。首先想到的自然是找工作。但是好巧不巧，今年的形势出奇的差:首先是今年互联网大寒冬，各大公司大裁员；其次是今年hc大幅下降，招收的人数少了很多；然后就是本人准备考研去了，对找工作的技能项目方面没有做好任何准备，这个才是最关键的。那就考公吧，家里人的意思就是这样的，我寻思了一下，好像也只能这样，然后我就等着我阳康之后就开始准备了。我个人其实是不想考公务员的，但是现实让我不得不这么做。我准备了一个来月，行测可以做到70多一些，申论就是听天由命了。在复习过程中，我还在反复地精神内耗，只能说很痛苦。2月25日，我参加了我人生中第一次公务员考试，也是我在高考后第一次写一篇考试作文。行测可以说差强人意，申论那更是勉勉强强了。考完了就先放一边，等出成绩再做打算。然而这些都只是权宜之计，真正的目的还是为了服务二战。</p><p>本人考研考的是某华5，11408。这次考的非常低，可能国家线都没有，比别人裸考都要低，真是搞笑。这次我还是决定要再冲一次，无论如何。本人的目标很单纯，就是想看看博士是什么样，给这些东西祛魅，然后做一点微不足道的贡献吧。其次就是想润出去看看，本人观念跟父母冲突很大，不想屈从于父母意志，也不想被宣传机器洗脑，还有就是对现实的不满。这些就是我现在的动力。但是，这次考的很差，而且对手都很强，加之经济形势不好，越来越多的人加入考研。二战压力很大，这也是我精神内耗的原因之一。二战的事情之后再说，还有10个月做准备。本人已经复习过一轮了，第二轮只能拿出老命了。</p><p>此外，呆在家里压力很大。俗话说，距离产生美。一回家，自己妈妈的态度就一百八十度大转弯。这时候我才看明白，原来我妈是不想让我考研的，虽然她表面上是支持我考，暗地里只是想让我考个公呆家里一辈子。但是我爹还是很赞成我二战的，但是还是要我先考公再说，毕竟还是有个工作稳妥些。说是这么说，真呆在家里只会是压力拉满，什么娱乐放松都不能有(虽然今年多半是不会有什么了)，还有天天言语上的压力，再加上社会自己同辈，考上只会是难上加难。这也是我精神内耗的来源之一。</p><p>但是，现在还是要有实际行动，虽然天天看牛客陌陌压力很大，可还是要准备工作啊，不能因为啥都没学就不去尝试。所以我打算先花点时间去学习一点java类后端的知识，先看情况找个实习做做，这样一能积累经验，二万一二战没上也能找工作，毕竟凡事都是说不准的。寒假自己拿着模板写了一份简历，但是发现自己没什么可写的，之前也没有专门为找工作而准备过，所以看看能不能上实习。</p><p>到现在总结一下，目前就是分三个方向，考公，就业，二战。前两者都是为第三者准备，第三者才是目前我认为的重中之重。其实现实的困难算是其次，最主要的困难是自己精神上的内耗：怕应届生这个好身份找不到什么好工作，怕自己没能力找工作，怕自己二战上不了岸，又看到别人光鲜亮丽，反正就是瞻前顾后，无所适从。天天刷知乎，但是知乎上的消息源又是压力来源之一。加上自己旁边一起考研的发挥很好，基本上岸，那就显得更慌了。这段时间算是我从小到大最失败的一段时间，也是最难过的一段时间。到现在，我明白了，要想过去，就只能熬，啥都不想，停下内耗，孤注一掷，反正已经是最差了也没什么可怕了。毕竟有句话说得好，不想赢就不会输。只有往前看才行，不管现在形势如何，硬着头皮也得上。</p><p>至于具体如何行动，可以分为三个部分。考公不必说，这个等出成绩通知面试再谈。找工作先去想办法润色一下简历，然后背一背八股学一些技术，找要求低一些，薪资过得去的公司面试，或者就直接实习。考研嘛，上面说了，压力拉满，所以最好是出去考研。其实还有个机会，就是出国，但是家里拿不出这么多钱，而且我没有选择刷绩点，所以也没戏。其次最主要的就是保持好心态，改掉自己的坏习惯，反思一下自己为什么会没考上，然后戒掉无用社交，一心一意拿下考研。还有就是多跟有经验的人交流，参考参考别人给的意见。</p><p>总之接下来会是一段非常难过的日子，希望我能够挨过去，拿到自己想要的东西。如果有人无意看到了这个，请吸取教训，要么直接工作，要么直接梭哈，不看别人任何消息，每年都会有重复这样悲剧的，希望不是看到这个的人。也希望自己一年后是笑着看到这个东西的。</p><p>随便写写，逻辑有点混乱，就这样凑合吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2022年12月25日，青岛开发区四中，一年一度的全国研究生考试开始了。青岛刺骨的寒风，没有温度的太阳。一场猝不及防的新冠，以及对数学的恐惧，不出意外，考砸了，没书读了。但是个人还是无法接受这个结果，虽然明知是自己自作自受:前三年如果博一把，就不用遭这门罪;如果再对自己狠点</summary>
      
    
    
    
    <category term="杂言" scheme="https://frankho-hwc.github.io/categories/%E6%9D%82%E8%A8%80/"/>
    
    
    <category term="工作" scheme="https://frankho-hwc.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
    <category term="考研" scheme="https://frankho-hwc.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
    <category term="碎碎念" scheme="https://frankho-hwc.github.io/tags/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    
    <category term="人生" scheme="https://frankho-hwc.github.io/tags/%E4%BA%BA%E7%94%9F/"/>
    
  </entry>
  
  <entry>
    <title>这是一个实验文章</title>
    <link href="https://frankho-hwc.github.io/2023/02/13/%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AE%9E%E9%AA%8C%E6%96%87%E7%AB%A0/"/>
    <id>https://frankho-hwc.github.io/2023/02/13/%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AE%9E%E9%AA%8C%E6%96%87%E7%AB%A0/</id>
    <published>2023-02-13T03:20:49.000Z</published>
    <updated>2023-02-13T03:42:59.862Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个实验文章</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是一个实验文章&lt;/p&gt;
</summary>
      
    
    
    
    <category term="exp" scheme="https://frankho-hwc.github.io/categories/exp/"/>
    
    
    <category term="实验" scheme="https://frankho-hwc.github.io/tags/%E5%AE%9E%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://frankho-hwc.github.io/2023/02/11/hello-world/"/>
    <id>https://frankho-hwc.github.io/2023/02/11/hello-world/</id>
    <published>2023-02-11T07:14:32.696Z</published>
    <updated>2023-02-11T07:12:45.531Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
