<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WelCome!</title>
  
  <subtitle>[object Object]</subtitle>
  <link href="https://frankho-hwc.github.io/atom.xml" rel="self"/>
  
  <link href="https://frankho-hwc.github.io/"/>
  <updated>2023-03-12T08:57:26.008Z</updated>
  <id>https://frankho-hwc.github.io/</id>
  
  <author>
    <name>Frank Ho</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>常见设计模式</title>
    <link href="https://frankho-hwc.github.io/2023/03/11/%E5%B8%B8%E8%A7%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>https://frankho-hwc.github.io/2023/03/11/%E5%B8%B8%E8%A7%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</id>
    <published>2023-03-11T08:55:52.000Z</published>
    <updated>2023-03-12T08:57:26.008Z</updated>
    
    <content type="html"><![CDATA[<h1 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h1><h1 id="设计模式分类"><a href="#设计模式分类" class="headerlink" title="设计模式分类"></a>设计模式分类</h1><p>设计模式分为三类： </p><h2 id="创造型模式："><a href="#创造型模式：" class="headerlink" title="创造型模式："></a>创造型模式：</h2><p>单例模式、⼯⼚模式、建造者模式、原型模式 </p><h2 id="结构型模式："><a href="#结构型模式：" class="headerlink" title="结构型模式："></a>结构型模式：</h2><p>适配器模式、桥接模式、外观模式、组合模式、装饰模式、享元模式、代理模式 </p><h2 id="⾏为型模式："><a href="#⾏为型模式：" class="headerlink" title="⾏为型模式："></a>⾏为型模式：</h2><p>责任链模式、命令模式、解释器模式、迭代器模式、中介者模式、备忘录模式、观察者模式、状态模 式、策略模式、模板⽅法模式、访问者模式。</p><h1 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h1><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>单例模式是指在整个系统生命周期内，保证一个类只能产生一个实例，确保该类的唯一性。</p><p>注意:</p><ul><li>1、单例类只能有一个实例。</li><li>2、单例类必须自己创建自己的唯一实例。</li><li>3、单例类必须给所有其他对象提供这一实例。</li></ul><p><strong>意图：</strong>保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p><p><strong>主要解决：</strong>一个全局使用的类频繁地创建与销毁。</p><p><strong>何时使用：</strong>当您想控制实例数目，节省系统资源的时候。</p><p><strong>如何解决：</strong>判断系统是否已经有这个单例，如果有则返回，如果没有则创建。</p><p><strong>关键代码：</strong>构造函数是私有的。</p><p><strong>应用例子：</strong> </p><ol><li>一个班级只有一个班主任</li><li>有些设备管理器只能设计为单例模式，如一台电脑有两个打印机，但是输出的时候只能有一台打印机打印文件。</li></ol><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li>在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。</li><li>避免对资源的多重占用（比如写文件操作）。</li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>1、要求生产唯一序列号。</li><li>2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。</li><li>3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。</li></ul><h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><h3 id="饿汉式-线程安全"><a href="#饿汉式-线程安全" class="headerlink" title="饿汉式(线程安全)"></a>饿汉式(线程安全)</h3><p><strong>是否 Lazy 初始化：</strong>否</p><p><strong>是否多线程安全：</strong>是</p><p><strong>实现难度：</strong>易</p><p>饿汉，顾名思义就是饿了就饥不择食，所以单例类定义的时候就进行了实例化。</p><p>在最开始的时候静态对象就已经创建完成，设计⽅法是类中包含⼀个静态成员指针，该指针指向该类的⼀个对象， 提供⼀个公有的静态成员⽅法，返回该对象指针，为了使得对象唯⼀，构造函数设为私有。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SingleInstance</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="function"><span class="type">static</span> SingleInstance* <span class="title">GetInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> <span class="type">static</span> SingleInstance ins;</span><br><span class="line"> <span class="keyword">return</span> &amp;ins;</span><br><span class="line"> &#125;</span><br><span class="line"> ~<span class="built_in">SingleInstance</span>()&#123;&#125;;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"> <span class="comment">//涉及到创建对象的函数都设置为private</span></span><br><span class="line"> <span class="built_in">SingleInstance</span>() &#123; std::cout&lt;&lt;<span class="string">&quot;SingleInstance() 饿汉&quot;</span>&lt;&lt;std::endl; &#125;</span><br><span class="line"> <span class="built_in">SingleInstance</span>(<span class="type">const</span> SingleInstance&amp; other) &#123;&#125;;</span><br><span class="line"> SingleInstance&amp; <span class="keyword">operator</span>=(<span class="type">const</span> SingleInstance&amp; other) &#123;<span class="keyword">return</span> *<span class="keyword">this</span>;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"> <span class="comment">//因为不能创建对象所以通过静态成员函数的⽅法返回静态成员变量</span></span><br><span class="line"> SingleInstance* ins = SingleInstance::<span class="built_in">GetInstance</span>();</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出 SingleInstance() 饿汉</span></span><br></pre></td></tr></table></figure><h3 id="懒汉式-线程安全"><a href="#懒汉式-线程安全" class="headerlink" title="懒汉式(线程安全)"></a>懒汉式(线程安全)</h3><p><strong>是否 Lazy 初始化：</strong>是</p><p><strong>是否多线程安全：</strong>是</p><p><strong>实现难度：</strong>易</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pthread.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SingleInstance</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">static</span> SingleInstance* <span class="title">GetInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> <span class="keyword">if</span> (ins == <span class="literal">nullptr</span>) &#123;</span><br><span class="line"> <span class="built_in">pthread_mutex_lock</span>(&amp;mutex);</span><br><span class="line"> <span class="keyword">if</span> (ins == <span class="literal">nullptr</span>) &#123;</span><br><span class="line"> ins = <span class="keyword">new</span> <span class="built_in">SingleInstance</span>();</span><br><span class="line">   &#125;</span><br><span class="line"> <span class="built_in">pthread_mutex_unlock</span>(&amp;mutex);</span><br><span class="line">        &#125;</span><br><span class="line"> <span class="keyword">return</span> ins;</span><br><span class="line"> &#125;</span><br><span class="line"> ~<span class="built_in">SingleInstance</span>()&#123;&#125;;</span><br><span class="line">  <span class="comment">//互斥锁</span></span><br><span class="line"> <span class="type">static</span> <span class="type">pthread_mutex_t</span> mutex;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"> <span class="comment">//涉及到创建对象的函数都设置为private</span></span><br><span class="line"> <span class="built_in">SingleInstance</span>() &#123; std::cout&lt;&lt;<span class="string">&quot;SingleInstance() 懒汉&quot;</span>&lt;&lt;std::endl; &#125;</span><br><span class="line"> <span class="built_in">SingleInstance</span>(<span class="type">const</span> SingleInstance&amp; other) &#123;&#125;;</span><br><span class="line"> SingleInstance&amp; <span class="keyword">operator</span>=(<span class="type">const</span> SingleInstance&amp; other) &#123; <span class="keyword">return</span> *<span class="keyword">this</span>; &#125;</span><br><span class="line"> <span class="comment">//静态成员</span></span><br><span class="line"> <span class="type">static</span> SingleInstance* ins;</span><br><span class="line">    </span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//懒汉式 静态变量需要定义</span></span><br><span class="line">SingleInstance* SingleInstance::ins = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="type">pthread_mutex_t</span> SingleInstance::mutex;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"> <span class="comment">//因为不能创建对象所以通过静态成员函数的⽅法返回静态成员变ᰁ</span></span><br><span class="line"> SingleInstance* ins = SingleInstance::<span class="built_in">GetInstance</span>();</span><br><span class="line"> <span class="keyword">delete</span> ins;</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出 SingleInstance() 懒汉</span></span><br></pre></td></tr></table></figure><h3 id="懒汉式-线程不安全"><a href="#懒汉式-线程不安全" class="headerlink" title="懒汉式(线程不安全)"></a>懒汉式(线程不安全)</h3><p><strong>是否 Lazy 初始化：</strong>是</p><p><strong>是否多线程安全：</strong>是</p><p><strong>实现难度：</strong>易</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SingleInstance</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="type">static</span> SingleInstance* instance;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="built_in">SingleInstance</span>() &#123;&#125;;</span><br><span class="line">~<span class="built_in">SingleInstance</span>() &#123;&#125;;</span><br><span class="line"><span class="built_in">SingletInstance</span>(<span class="type">const</span> SingleInstance&amp;);</span><br><span class="line">SingleInstance&amp; <span class="keyword">operator</span>=(<span class="type">const</span> SingleInstance&amp;);</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">static</span> SingleInstance* <span class="title">getInstance</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">        </span>&#123;</span><br><span class="line"><span class="keyword">if</span>(instance == <span class="literal">NULL</span>) </span><br><span class="line">instance = <span class="keyword">new</span> <span class="built_in">SingleInstance</span>();</span><br><span class="line"><span class="keyword">return</span> instance;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// init static member</span></span><br><span class="line">SingleInstance* SingleInstance::ins = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"> <span class="comment">//因为不能创建对象所以通过静态成员函数的⽅法返回静态成员变ᰁ</span></span><br><span class="line"> SingleInstance* ins = SingleInstance::<span class="built_in">GetInstance</span>();</span><br><span class="line"> <span class="keyword">delete</span> ins;</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="工厂模式"><a href="#工厂模式" class="headerlink" title="工厂模式"></a>工厂模式</h1><h3 id="简单工厂模式"><a href="#简单工厂模式" class="headerlink" title="简单工厂模式"></a>简单工厂模式</h3><p>就是建⽴⼀个⼯⼚类，对实现了同⼀接⼝的⼀些类进⾏实例的创建。简单⼯⼚模式的实质是由⼀个⼯⼚类根据传⼊ 的参数，动态决定应该创建哪⼀个产品类（这些产品类继承⾃⼀个⽗类或接⼝）的实例。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line">⼯⼚模式⽬的就是代码解耦，如果我们不采⽤⼯⼚模式，如果要创建产品 A、B，通常做法采⽤⽤ <span class="keyword">switch</span>...<span class="keyword">case</span>语 句，那么想⼀想后期添加更多的产品进来，我们不是要添加更多的 <span class="keyword">switch</span>...<span class="keyword">case</span> 吗？这样就很麻烦，⽽且也不符 合设计模式中的开放封闭原则。</span><br><span class="line"></span><br><span class="line">### 抽象工厂模式 </span><br><span class="line"></span><br><span class="line"># 装饰器模式</span><br><span class="line"></span><br><span class="line">装饰器模式（Decorator Pattern）允许向⼀个现有的对象添加新的功能，同时⼜不改变其结构。 这种类型的设计模式属于结构型模式，它是作为现有的类的⼀个包装。 代码没有改变 Car 类的内部结构，还为其增加了新的功能，这就是装饰器模式的作⽤。</span><br><span class="line"></span><br><span class="line">```c++</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">//抽象构件类 Transform (变形⾦刚)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transform</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">move</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//具体构件类Car</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Car</span> : <span class="keyword">public</span> Transform&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="built_in">Car</span>()&#123;</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;变形⾦刚是⼀辆⻋！&quot;</span> &lt;&lt; endl;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">move</span><span class="params">()</span></span>&#123;</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;在陆地上移动。&quot;</span> &lt;&lt; endl;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//抽象装饰类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Changer</span> : <span class="keyword">public</span> Transform&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="built_in">Changer</span>(shared_ptr&lt;Transform&gt; transform)&#123;</span><br><span class="line"> <span class="keyword">this</span>-&gt;transform = transform;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">move</span><span class="params">()</span></span>&#123;</span><br><span class="line"> transform-&gt;<span class="built_in">move</span>();</span><br><span class="line"> &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"> shared_ptr&lt;Transform&gt; transform;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//具体装饰类Robot</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Robot</span> : <span class="keyword">public</span> Changer&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="built_in">Robot</span>(shared_ptr&lt;Transform&gt; transform) : <span class="built_in">Changer</span>(transform)&#123;</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;变成机器⼈!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">say</span><span class="params">()</span></span>&#123;</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;说话!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//具体装饰类AirPlane</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Airplane</span> : <span class="keyword">public</span> Changer&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="built_in">Airplane</span>(shared_ptr&lt;Transform&gt; transform) : <span class="built_in">Changer</span>(transform)&#123;</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;变成⻜机!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">say</span><span class="params">()</span></span>&#123;</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;在天空⻜翔!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"> &#125; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span>&#123;</span><br><span class="line"> shared_ptr&lt;Transform&gt; camaro = <span class="built_in">make_shared</span>&lt;Car&gt;();</span><br><span class="line"> camaro-&gt;<span class="built_in">move</span>();</span><br><span class="line"> std::cout &lt;&lt; <span class="string">&quot;--------------&quot;</span> &lt;&lt; endl;</span><br><span class="line"> shared_ptr&lt;Robot&gt; bumblebee = <span class="built_in">make_shared</span>&lt;Robot&gt;(camaro);</span><br><span class="line"> bumblebee-&gt;<span class="built_in">move</span>();</span><br><span class="line"> bumblebee-&gt;<span class="built_in">say</span>();</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="观察者模式"><a href="#观察者模式" class="headerlink" title="观察者模式"></a>观察者模式</h1><p>观察者模式：定义⼀种⼀（被观察类）对多（观察类）的关系，让多个观察对象同时监听⼀个被观察对象，被观察 对象状态发⽣变化时，会通知所有的观察对象，使他们能够更新⾃⼰的状态。</p><p>观察者模式中存在两种⻆⾊： </p><p><strong>观察者</strong>：内部包含被观察者对象，当被观察者对象的状态发⽣变化时，更新⾃⼰的状态。（接收通知更新状态） </p><p><strong>被观察者</strong>：内部包含了所有观察者对象，当状态发⽣变化时通知所有的观察者更新⾃⼰的状态。（发送通知） </p><p><strong>应⽤场景</strong>： 当⼀个对象的改变需要同时改变其他对象，且不知道具体有多少对象有待改变时，应该考虑使⽤观察者模式； ⼀个抽象模型有两个⽅⾯，其中⼀⽅⾯依赖于另⼀⽅⾯，这时可以⽤观察者模式将这两者封装在独⽴的对象中使它 们各⾃独⽴地改变和复⽤。</p><p>实现如下</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Subject</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Observer</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">protected</span>: </span><br><span class="line">        string   name;</span><br><span class="line">        Subject  *sub;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Observer</span>(string name, Subject *sub)</span><br><span class="line">        &#123;</span><br><span class="line">             <span class="keyword">this</span> -&gt; name = name;</span><br><span class="line">             <span class="keyword">this</span> -&gt; sub  = sub;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">update</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StockObserver</span> : <span class="keyword">public</span> Observer</span><br><span class="line">&#123;</span><br><span class="line">     <span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">StockObserver</span>(string name ,Subject *sub): <span class="built_in">Observer</span>(name,sub)&#123;&#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">update</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NBAObserver</span> : <span class="keyword">public</span> Observer &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> <span class="built_in">NBAObserver</span>(string name, Subject *sub) : <span class="built_in">Observer</span>(name, sub)&#123;&#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">update</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Subject</span> &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"> std::list&lt;Observer *&gt; observers;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"> string action; <span class="comment">//被观察者对象的状态</span></span><br><span class="line"> <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">attach</span><span class="params">(Observer *)</span> </span>= <span class="number">0</span>;</span><br><span class="line"> <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">detach</span><span class="params">(Observer *)</span> </span>= <span class="number">0</span>;</span><br><span class="line"> <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">notify</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Secretary</span> : <span class="keyword">public</span> Subject &#123;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">attach</span><span class="params">(Observer *observer)</span> </span>&#123;</span><br><span class="line"> observers.<span class="built_in">push_back</span>(observer);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">detach</span><span class="params">(Observer *observer)</span> </span>&#123;</span><br><span class="line"> list&lt;Observer *&gt;::iterator iter = observers.<span class="built_in">begin</span>();</span><br><span class="line"> <span class="keyword">while</span> (iter != observers.<span class="built_in">end</span>()) &#123;</span><br><span class="line"> <span class="keyword">if</span> ((*iter) == observer) &#123;</span><br><span class="line"> observers.<span class="built_in">erase</span>(iter);</span><br><span class="line"> <span class="keyword">return</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> ++iter;</span><br><span class="line"> &#125;     </span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="type">void</span> <span class="title">notify</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> list&lt;Observer *&gt;::iterator iter = observers.<span class="built_in">begin</span>();</span><br><span class="line"> <span class="keyword">while</span> (iter != observers.<span class="built_in">end</span>()) &#123;</span><br><span class="line"> (*iter)-&gt;<span class="built_in">update</span>();</span><br><span class="line"> ++iter;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">StockObserver::update</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> cout &lt;&lt; name &lt;&lt; <span class="string">&quot; 收到消息：&quot;</span> &lt;&lt; sub-&gt;action &lt;&lt; endl;</span><br><span class="line"> <span class="keyword">if</span> (sub-&gt;action == <span class="string">&quot;老板来了!&quot;</span>) &#123;</span><br><span class="line"> cout &lt;&lt; <span class="string">&quot;我关上关闭股票，装做很认真工作的样子！&quot;</span> &lt;&lt; endl;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">NBAObserver::update</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> cout &lt;&lt; name &lt;&lt; <span class="string">&quot; 收到消息：&quot;</span> &lt;&lt; sub-&gt;action &lt;&lt; endl;</span><br><span class="line"> <span class="keyword">if</span> (sub-&gt;action == <span class="string">&quot;老板来了!&quot;</span>) &#123;</span><br><span class="line"> cout &lt;&lt; <span class="string">&quot;我关上关闭 NBA，装做很认真工作的样子！&quot;</span> &lt;&lt; endl;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> Subject *BOSS = <span class="keyword">new</span> <span class="built_in">Secretary</span>();</span><br><span class="line"> Observer *xa = <span class="keyword">new</span> <span class="built_in">NBAObserver</span>(<span class="string">&quot;xa&quot;</span>, BOSS);</span><br><span class="line"> Observer *xb = <span class="keyword">new</span> <span class="built_in">NBAObserver</span>(<span class="string">&quot;xb&quot;</span>, BOSS);</span><br><span class="line"> Observer *xc = <span class="keyword">new</span> <span class="built_in">StockObserver</span>(<span class="string">&quot;xc&quot;</span>, BOSS);</span><br><span class="line"> BOSS-&gt;<span class="built_in">attach</span>(xa);</span><br><span class="line"> BOSS-&gt;<span class="built_in">attach</span>(xb);</span><br><span class="line"> BOSS-&gt;<span class="built_in">attach</span>(xc);</span><br><span class="line"> BOSS-&gt;action = <span class="string">&quot;去吃饭了！&quot;</span>;</span><br><span class="line"> BOSS-&gt;<span class="built_in">notify</span>();</span><br><span class="line"> cout &lt;&lt; endl;</span><br><span class="line"> BOSS-&gt;action = <span class="string">&quot;老板来了!&quot;</span>;</span><br><span class="line"> BOSS-&gt;<span class="built_in">notify</span>();</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="comment">//product A create! product B create!</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;设计模式&quot;&gt;&lt;a href=&quot;#设计模式&quot; class=&quot;headerlink&quot; title=&quot;设计模式&quot;&gt;&lt;/a&gt;设计模式&lt;/h1&gt;&lt;h1 id=&quot;设计模式分类&quot;&gt;&lt;a href=&quot;#设计模式分类&quot; class=&quot;headerlink&quot; title=&quot;设计模式分</summary>
      
    
    
    
    
    <category term="工作" scheme="https://frankho-hwc.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
    <category term="c++" scheme="https://frankho-hwc.github.io/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>图像质量评价指标</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</id>
    <published>2023-03-09T14:17:14.000Z</published>
    <updated>2023-03-11T12:47:25.374Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>图像质量评估(Image Quality Assessment)对于计算机视觉任务内是很重要的一部分，根据是否是以人的主观观察还是客观标准可以分为主观评价指标和客观评价指标。其中图像质量客观评价可分为全参考（Full-Reference,FR），部分参考（Reduced-Reference,RR）和无参考（No-Reference,NR）三种类型。</p><h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><h2 id="全参考"><a href="#全参考" class="headerlink" title="全参考"></a>全参考</h2><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p>平均绝对误差,预测值与真实值的绝对误差的平均值</p><script type="math/tex; mode=display">MAE(y,\hat y) = \frac{1}{MN}\sum_{i=0}^{M}\sum_{j = 0}^{N}|x(i,j) - \hat x (i,j)|</script><h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><p>均方误差(Mean Square Error)，预测值与真实值的绝对平方误差的平均值,实际上就是两幅图像的所有的像素值的差的平方和，再求平均值，最终得到的是一个数值。</p><script type="math/tex; mode=display">MSE(y,\hat y) = \frac{1}{MN}\sum_{i=0}^{M}\sum_{j = 0}^{N}[x(i,j) - \hat x (i,j)]^2</script><h3 id="SNR"><a href="#SNR" class="headerlink" title="SNR"></a>SNR</h3><p>给定一个给定一个大小为m×n的灰度图 I 和噪声图 K，已知其MSE，定义SNR如下</p><script type="math/tex; mode=display">SNR = 10 \cdot \lg [\frac{\sum_{i=0}^{M}\sum_{j = 0}^{N}x^2(i,j)}{MSE}] (db)</script><h3 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h3><p>峰值信噪比(Peak Signal to Noise Ratio, PSNR)是一种评价图像质量的度量标准。因为PSNR值具有局限性，所以它只是衡量最大值信号和背景噪音之间的图像质量参考值。PSNR的单位为dB，其值越大，图像失真越少。一般来说，PSNR高于40dB说明图像质量几乎与原图一样好；在30-40dB之间通常表示图像质量的失真损失在可接受范围内；在20-30dB之间说明图像质量比较差；PSNR低于20dB说明图像失真严重。</p><p>给定一个给定一个大小为m×n的灰度图 I 和噪声图 K，已知其MSE，定义PSNR如下</p><script type="math/tex; mode=display">PSNR = 10 \cdot \log_{10}(\frac{MAX_I^2}{MSE}) (db)</script><p>其中，$MAX_I$为图片可能的最大像素值，即B-bit的图像的$MAX_I$值为 $2^B-1$。一般地，针对 uint8 数据，最大像素值为255；针对浮点型数据，最大像素值为1。</p><p>有三种方法来计算彩色RGB图像的PSNR：</p><ol><li>分别计算 RGB 三个通道的 PSNR，然后取平均值；</li><li>或者计算RGB各个通道的均方差的均值，然后统一求PSNR；</li><li>或者把RGB转化为 YCbCr，然后只计算 Y(亮度)分量的PSNR。</li></ol><h3 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h3><p>结构相似性指数（structural similarity index，SSIM）是一种用于量化两幅图像间的结构相似性的指标。与L2损失函数不同，SSIM仿照人类的视觉系统（Human Visual System,HVS）实现了结构相似性的有关理论，对图像的局部结构变化的感知敏感。SSIM从亮度、对比度以及结构量化图像的属性，用均值估计亮度，方差估计对比度，协方差估计结构相似程度。SSIM值的范围为0至1，越大代表图像越相似。如果两张图片完全一样时，SSIM值为1。</p><p>给定x,y两张图片，两者之间的照明度(luminance)、对比度 (contrast) 和结构 (structure)分别如下公式所示：</p><script type="math/tex; mode=display">\begin{align}l(x,y) =& \frac{2 \mu_x\mu_y  + c_1}{\mu_x^2+\mu_y^2+c_1}\\c(x,y) =& \frac{2\sigma_x\sigma_y+c_2}{\sigma_x^2+\sigma_y^2+c_2}\\s(x,y) =& \frac{\sigma_{xy}+c_3}{\sigma_x\sigma_y+c_3}\end{align}</script><p>定义</p><script type="math/tex; mode=display">SSIM(x,y) = [l(x,y)^\alpha \cdot c(x,y)^\beta \cdot s(x,y)^\gamma]</script><p>令$\alpha,\beta,\gamma$ 均为1，则SSIM表达式为</p><script type="math/tex; mode=display">SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2+\sigma_y^2+c_2)}</script><p>其中， $\mu_x$ </p><p> 是x的平均值，$\sigma<em>x^2$  是x的方差, $\mu_y$ 是y的平均值，$\sigma_y^2$ 是y的方差，$\sigma</em>{xy}$ 是x 和 y 的协方差， $c_1 = (k_1L)^2,c_2 = (k_2L)^2$  是两个用于维持稳定的常数，避免出现除零的情况，L 为像素值的范围，表示B-bit的图像的L值为  $2^B-1$ 。一般地，针对 uint8 数据，最大像素值为255。针对浮点型数据，最大像素值为1。一般情况下，k1=0.01,k2=0.03。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>PSNR与MSE都是通过计算待评图像与参考图像之间像素误差的全局大小来衡量图像质量好坏的。PSNR值越大，表明待评图像与参考图像之间的失真较小，图像质量较好。而MSE的值越小，表明图像质量越好。这两种方法比较简单，且容易实现，在图像去噪等方面受到广泛应用。但这类算法是从图像像素值的全局统计出发，未考虑人眼的局部视觉因素，所以对于图像局部质量无从把握。</p><h3 id="IFC"><a href="#IFC" class="headerlink" title="IFC"></a>IFC</h3><p>信息保真度准则（IFC）</p><h3 id="VIF"><a href="#VIF" class="headerlink" title="VIF"></a>VIF</h3><p>算法介绍：<a href="https://live.ece.utexas.edu/research/Quality/VIF.htm">https://live.ece.utexas.edu/research/Quality/VIF.htm</a></p><p>视觉信息保真度（Visual Information Fidelity,VIF），是由德克萨斯大学奥斯汀分校的图像和视频工程实验室（<em>LIVE: Laboratory for Image and Video Engineering</em>）提出的算法。</p><p>VIF算法将图像/视频的质量评估问题看作一个信息保真的问题，并从信息通信和共享的角度来评估图像/视频的质量。</p><p>VIF认为：</p><ul><li>人眼所看到的图像是通过HVS过滤之后而提取到的信息，在这个过程中，人类视觉系统也是一个简单的失真通道</li><li>在经过HVS之前，失真图像只是比原始图像多经过了一个失真通道</li><li>利用信息论的理论，把人眼从失真图像中提取到的信息和人眼从原始图像中提取到的信息进行对比，从而得到符合人主观感知的图像质量</li></ul><p>整体VIF算法中包含三个部分：<code>Source Model</code>，<code>Distortion Model</code>，<code>HVS Model</code>，具体如下图所示：</p><p><img src="/2023/03/09/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/1.gif" alt="img"></p><p>具体内容可见<a href="https://wangwei1237.github.io/2021/03/05/intoduction-to-VIF/#:~:text=视觉信息保真度（VIF）是基于 自然场景统计（ natural scene statistics ）,和 人类视觉系统（ human visual system ） 提取图像信息的一种全参考的图像质量评估指标，并且与人类对视觉质量的判断具有良好的相关性。">VIF质量评估方法简介 | 17哥 (wangwei1237.github.io)</a>，该文章较为详细地介绍了VIF算法如何进行图像质量评价的。</p><h2 id="部分参考"><a href="#部分参考" class="headerlink" title="部分参考"></a>部分参考</h2><h2 id="无参考"><a href="#无参考" class="headerlink" title="无参考"></a>无参考</h2><p>无参考方法也称为首评价方法，因为一般的理想图像很难获得，所以这种完全脱离了对理想参考图像依赖的质量评价方法应用较为广泛。无参考方法一般都是基于图像统计特性。</p><h3 id="均值"><a href="#均值" class="headerlink" title="均值"></a>均值</h3><p>标准差是指图像像素灰度值相对于均值的离散程度。如果标准差越大，表明图像中灰度级分别越分散，图像质量也就越好，其计算公式为：</p><script type="math/tex; mode=display">u= \frac{1}{MN}\sum_{i= 1}^{M}\sum_{j=1}^{N}F(i,j)</script><h3 id="标准差"><a href="#标准差" class="headerlink" title="标准差"></a>标准差</h3><p>标准差是指图像像素灰度值相对于均值的离散程度。如果标准差越大，表明图像中灰度级分别越分散，图像质量也就越好，其计算公式为：</p><script type="math/tex; mode=display">std = \frac{1}{MN}\sum_{i= 1}^{M}\sum_{j=1}^{N}\sqrt{(F(i,j) - u)^2}</script><h3 id="平均梯度"><a href="#平均梯度" class="headerlink" title="平均梯度"></a>平均梯度</h3><p>平均梯度：反应图像中细节反差和纹理变化，他在一定程度上反应图像的清晰度，公式如下</p><script type="math/tex; mode=display">\nabla G = \frac{1}{MN}\sum_{i= 1}^{M}\sum_{j=1}^{N}\sqrt{\Delta xF(i,j)^2+\Delta y F(i,j)^2}</script><p>其中，$\Delta xF(i,j)^2$表示像素点(i,j)在x上的一阶差分,$\Delta yF(i,j)^2$表示该点在y方向上的一阶差分。</p><h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>指图像的平均信息量，从信息论的角度衡量图像中信息的多少，信息熵越大，说明图像包含的信息越多。假设图像中各个像素点的灰度值之间时相互独立的，图像的灰度分布为p={p1,p2,…..,pi,…….pn}，其中Pi表示灰度值为i的像素的个数与总像素个数之比，而n为灰度级总数，则计算公式为</p><script type="math/tex; mode=display">H =  -\sum_{i=0}^{255}p_i \log    p_i</script><p>其中，$p_i$是某个灰度在该图像中出现的概率，可由灰度直方图获得。</p><h3 id="NIQE-自然图像评估"><a href="#NIQE-自然图像评估" class="headerlink" title="NIQE(自然图像评估)"></a>NIQE(自然图像评估)</h3><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p><a href="https://zhuanlan.zhihu.com/p/309892873">有真实参照的图像质量的客观评估指标:SSIM、PSNR和LPIPS - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/qq_27825451/article/details/102954096">(41条消息) 图像质量评估各项指标（一）_图像的评价指标_LoveMIss-Y的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_23304241/article/details/80953613">(41条消息) 图像质量评价概述（评估指标、传统检测方法）_图像主观评价标准有哪些_qq_23304241的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/bby1987/article/details/109373572">(41条消息) 图像质量评估指标：MSE，PSNR，SSIM<em>图像mse</em>拜阳的博客-CSDN博客</a>.</p><p><a href="https://wangwei1237.github.io/2021/03/05/intoduction-to-VIF/#:~:text=视觉信息保真度（VIF）是基于 自然场景统计（ natural scene statistics ）,和 人类视觉系统（ human visual system ） 提取图像信息的一种全参考的图像质量评估指标，并且与人类对视觉质量的判断具有良好的相关性。">VIF质量评估方法简介 | 17哥 (wangwei1237.github.io)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;图像质量评估(Image Quality Assessment)对于计算机视觉任务内是很重要的一部分，根据是否是以人的主观观察还是客观标准可</summary>
      
    
    
    
    
    <category term="低亮图像增强" scheme="https://frankho-hwc.github.io/tags/%E4%BD%8E%E4%BA%AE%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>c++八股文</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/c++%E5%85%AB%E8%82%A1%E6%96%87/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/c++%E5%85%AB%E8%82%A1%E6%96%87/</id>
    <published>2023-03-09T08:50:01.000Z</published>
    <updated>2023-03-11T12:37:56.504Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题-1-c-中内存的分布情况"><a href="#问题-1-c-中内存的分布情况" class="headerlink" title="问题(1) c++中内存的分布情况"></a>问题(1) c++中内存的分布情况</h1><h1 id="问题-2-栈和堆的区别"><a href="#问题-2-栈和堆的区别" class="headerlink" title="问题(2) 栈和堆的区别"></a>问题(2) 栈和堆的区别</h1><h1 id="问题-3-C-如何申请空间"><a href="#问题-3-C-如何申请空间" class="headerlink" title="问题(3) C++如何申请空间"></a>问题(3) C++如何申请空间</h1><h1 id="问题-4-C-如何申请空间"><a href="#问题-4-C-如何申请空间" class="headerlink" title="问题(4) C++如何申请空间"></a>问题(4) C++如何申请空间</h1><h1 id="问题-5-函数传递参数的几种方式"><a href="#问题-5-函数传递参数的几种方式" class="headerlink" title="问题(5) 函数传递参数的几种方式"></a>问题(5) 函数传递参数的几种方式</h1><h1 id="问题-6-函数传递参数的几种方式"><a href="#问题-6-函数传递参数的几种方式" class="headerlink" title="问题(6) 函数传递参数的几种方式"></a>问题(6) 函数传递参数的几种方式</h1><h1 id="问题-7-指针和引用的区别"><a href="#问题-7-指针和引用的区别" class="headerlink" title="问题(7) 指针和引用的区别"></a>问题(7) 指针和引用的区别</h1><h1 id="问题-8-指针和数组的区别"><a href="#问题-8-指针和数组的区别" class="headerlink" title="问题(8)  指针和数组的区别"></a>问题(8)  指针和数组的区别</h1><h1 id="问题-9-野指针和悬挂指针，并如何避免"><a href="#问题-9-野指针和悬挂指针，并如何避免" class="headerlink" title="问题(9) 野指针和悬挂指针，并如何避免"></a>问题(9) 野指针和悬挂指针，并如何避免</h1><h1 id="问题-10-const修饰指针"><a href="#问题-10-const修饰指针" class="headerlink" title="问题(10) const修饰指针"></a>问题(10) const修饰指针</h1><h1 id="问题-11-static定义静态变量"><a href="#问题-11-static定义静态变量" class="headerlink" title="问题(11) static定义静态变量"></a>问题(11) static定义静态变量</h1><h1 id="问题-12-const和volatile"><a href="#问题-12-const和volatile" class="headerlink" title="问题(12) const和volatile"></a>问题(12) const和volatile</h1><h1 id="问题-13-宏定义-define和常量const的区别"><a href="#问题-13-宏定义-define和常量const的区别" class="headerlink" title="问题(13) 宏定义#define和常量const的区别"></a>问题(13) 宏定义#define和常量const的区别</h1><h1 id="问题-14-C-中struct和class的区别"><a href="#问题-14-C-中struct和class的区别" class="headerlink" title="问题(14) C++中struct和class的区别"></a>问题(14) C++中struct和class的区别</h1><h1 id="问题-15-C和C-的区别"><a href="#问题-15-C和C-的区别" class="headerlink" title="问题(15) C和C++的区别"></a>问题(15) C和C++的区别</h1><h1 id="问题-16-C-C-编译过程"><a href="#问题-16-C-C-编译过程" class="headerlink" title="问题(16) C/C++编译过程"></a>问题(16) C/C++编译过程</h1><h1 id="问题-17-智能指针有哪些，作用是什么"><a href="#问题-17-智能指针有哪些，作用是什么" class="headerlink" title="问题(17) 智能指针有哪些，作用是什么"></a>问题(17) 智能指针有哪些，作用是什么</h1><h1 id="问题-18-C-的-STL-介绍（内存管理，allocator，函数，实现机理，多线程实-现等）"><a href="#问题-18-C-的-STL-介绍（内存管理，allocator，函数，实现机理，多线程实-现等）" class="headerlink" title="问题(18) C++ 的 STL 介绍（内存管理，allocator，函数，实现机理，多线程实 现等）"></a>问题(18) C++ 的 STL 介绍（内存管理，allocator，函数，实现机理，多线程实 现等）</h1><h1 id="问题-19-vector-使⽤的注意点及其原因，频繁对-vector-调⽤-push-back-性-能影响"><a href="#问题-19-vector-使⽤的注意点及其原因，频繁对-vector-调⽤-push-back-性-能影响" class="headerlink" title="问题(19) vector 使⽤的注意点及其原因，频繁对 vector 调⽤ push_back() 性 能影响"></a>问题(19) vector 使⽤的注意点及其原因，频繁对 vector 调⽤ push_back() 性 能影响</h1><h1 id="问题-20-map-和-set-有什么区别，分别⼜是怎么实现的？"><a href="#问题-20-map-和-set-有什么区别，分别⼜是怎么实现的？" class="headerlink" title="问题(20)map 和 set 有什么区别，分别⼜是怎么实现的？"></a>问题(20)map 和 set 有什么区别，分别⼜是怎么实现的？</h1><h1 id="问题-21-请你来说⼀说-STL-迭代器删除元素"><a href="#问题-21-请你来说⼀说-STL-迭代器删除元素" class="headerlink" title="问题(21)请你来说⼀说 STL 迭代器删除元素"></a>问题(21)请你来说⼀说 STL 迭代器删除元素</h1><h1 id="问题-22-请你来说⼀下-STL-中迭代器的作⽤，有指针为何还要迭代器"><a href="#问题-22-请你来说⼀下-STL-中迭代器的作⽤，有指针为何还要迭代器" class="headerlink" title="问题(22) 请你来说⼀下 STL 中迭代器的作⽤，有指针为何还要迭代器"></a>问题(22) 请你来说⼀下 STL 中迭代器的作⽤，有指针为何还要迭代器</h1><h1 id="问题-23-回答⼀下-STL-⾥-resize-和-reserve-的区别"><a href="#问题-23-回答⼀下-STL-⾥-resize-和-reserve-的区别" class="headerlink" title="问题(23) 回答⼀下 STL ⾥ resize 和 reserve 的区别"></a>问题(23) 回答⼀下 STL ⾥ resize 和 reserve 的区别</h1><h1 id="问题-24-C-强制类型转换有哪些"><a href="#问题-24-C-强制类型转换有哪些" class="headerlink" title="问题(24) C++强制类型转换有哪些"></a>问题(24) C++强制类型转换有哪些</h1><h1 id="问题-25-override-和-overload"><a href="#问题-25-override-和-overload" class="headerlink" title="问题(25) override 和 overload"></a>问题(25) override 和 overload</h1><h1 id="问题-26-C-和-Java-区别（语⾔特性，垃圾回收，应⽤场景等）"><a href="#问题-26-C-和-Java-区别（语⾔特性，垃圾回收，应⽤场景等）" class="headerlink" title="问题(26) C++ 和 Java 区别（语⾔特性，垃圾回收，应⽤场景等）"></a>问题(26) C++ 和 Java 区别（语⾔特性，垃圾回收，应⽤场景等）</h1><h1 id="问题-27-说⼀下-C-⾥是怎么定义常ᰁ的？常ᰁ存放在内存的哪个位置？"><a href="#问题-27-说⼀下-C-⾥是怎么定义常ᰁ的？常ᰁ存放在内存的哪个位置？" class="headerlink" title="问题(27) 说⼀下 C++ ⾥是怎么定义常ᰁ的？常ᰁ存放在内存的哪个位置？"></a>问题(27) 说⼀下 C++ ⾥是怎么定义常ᰁ的？常ᰁ存放在内存的哪个位置？</h1><h1 id="问题-28-介绍-C-所有的构造函数"><a href="#问题-28-介绍-C-所有的构造函数" class="headerlink" title="问题(28) 介绍 C++ 所有的构造函数"></a>问题(28) 介绍 C++ 所有的构造函数</h1><h1 id="问题-29-介绍-C-所有的构造函数"><a href="#问题-29-介绍-C-所有的构造函数" class="headerlink" title="问题(29) 介绍 C++ 所有的构造函数"></a>问题(29) 介绍 C++ 所有的构造函数</h1><h1 id="问题-30-类的继承权限问题"><a href="#问题-30-类的继承权限问题" class="headerlink" title="问题(30) 类的继承权限问题"></a>问题(30) 类的继承权限问题</h1><h1 id="问题-31-菱形继承问题"><a href="#问题-31-菱形继承问题" class="headerlink" title="问题(31) 菱形继承问题"></a>问题(31) 菱形继承问题</h1><h1 id="问题-32-C-中-lt-gt-和”-“引入头文件的区别"><a href="#问题-32-C-中-lt-gt-和”-“引入头文件的区别" class="headerlink" title="问题(32) C++中&lt;&gt;和” “引入头文件的区别"></a>问题(32) C++中&lt;&gt;和” “引入头文件的区别</h1><h1 id="问题-33-虚函数的作用：（virtual关键字修饰）"><a href="#问题-33-虚函数的作用：（virtual关键字修饰）" class="headerlink" title="问题(33) 虚函数的作用：（virtual关键字修饰）"></a>问题(33) 虚函数的作用：（virtual关键字修饰）</h1><h1 id="问题-34-哪些函数不能是虚函数"><a href="#问题-34-哪些函数不能是虚函数" class="headerlink" title="问题(34)  哪些函数不能是虚函数"></a>问题(34)  哪些函数不能是虚函数</h1><h1 id="问题-35-析构函数为什么要写成虚函数"><a href="#问题-35-析构函数为什么要写成虚函数" class="headerlink" title="问题(35) 析构函数为什么要写成虚函数"></a>问题(35) 析构函数为什么要写成虚函数</h1><h1 id="问题-36-纯虚函数和抽象类"><a href="#问题-36-纯虚函数和抽象类" class="headerlink" title="问题(36) 纯虚函数和抽象类"></a>问题(36) 纯虚函数和抽象类</h1><h1 id="问题-37-浅拷贝和深拷贝"><a href="#问题-37-浅拷贝和深拷贝" class="headerlink" title="问题(37) 浅拷贝和深拷贝"></a>问题(37) 浅拷贝和深拷贝</h1><h1 id="问题-38-调用拷贝构造函数的时机"><a href="#问题-38-调用拷贝构造函数的时机" class="headerlink" title="问题(38) 调用拷贝构造函数的时机"></a>问题(38) 调用拷贝构造函数的时机</h1><h1 id="问题-39-拷贝构造函数为什么要是引用传递，而不能是值传递"><a href="#问题-39-拷贝构造函数为什么要是引用传递，而不能是值传递" class="headerlink" title="问题(39) 拷贝构造函数为什么要是引用传递，而不能是值传递"></a>问题(39) 拷贝构造函数为什么要是引用传递，而不能是值传递</h1><h1 id="问题-40-define和typedef的区别"><a href="#问题-40-define和typedef的区别" class="headerlink" title="问题(40) #define和typedef的区别"></a>问题(40) #define和typedef的区别</h1><h1 id="问题-41-define和inline的区别"><a href="#问题-41-define和inline的区别" class="headerlink" title="问题(41) #define和inline的区别"></a>问题(41) #define和inline的区别</h1><h1 id="问题-42-explicit关键字"><a href="#问题-42-explicit关键字" class="headerlink" title="问题(42)  explicit关键字"></a>问题(42)  explicit关键字</h1><h1 id="问题-43-简单说⼀下函数指针"><a href="#问题-43-简单说⼀下函数指针" class="headerlink" title="问题(43) 简单说⼀下函数指针"></a>问题(43) 简单说⼀下函数指针</h1><h1 id="问题-44-volatile-和-extern-关键字"><a href="#问题-44-volatile-和-extern-关键字" class="headerlink" title="问题(44) volatile 和 extern 关键字"></a>问题(44) volatile 和 extern 关键字</h1><h1 id="问题-45-计算下⾯⼏个类的⼤⼩"><a href="#问题-45-计算下⾯⼏个类的⼤⼩" class="headerlink" title="问题(45) 计算下⾯⼏个类的⼤⼩"></a>问题(45) 计算下⾯⼏个类的⼤⼩</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;&#125;; <span class="built_in">sizeof</span>(A) = <span class="number">1</span>; <span class="comment">//空类在实例化时得到⼀个独⼀⽆⼆的地址，所以为 1.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;<span class="function"><span class="keyword">virtual</span> <span class="title">Fun</span><span class="params">()</span></span>&#123;&#125; &#125;; <span class="built_in">sizeof</span>(A) = <span class="number">4</span>(<span class="number">32b</span>it)/<span class="number">8</span>(<span class="number">64b</span>it) <span class="comment">//当 C++ 类中有虚函数的时候，会有⼀个指向虚函数表的指针（vptr）</span></span><br><span class="line"><span class="keyword">class</span> A&#123;<span class="type">static</span> <span class="type">int</span> a; &#125;; <span class="built_in">sizeof</span>(A) = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;<span class="type">int</span> a; &#125;; <span class="built_in">sizeof</span>(A) = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span>&#123;<span class="type">static</span> <span class="type">int</span> a; <span class="type">int</span> b; &#125;; <span class="built_in">sizeof</span>(A) = <span class="number">4</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="问题-46-多态的实现"><a href="#问题-46-多态的实现" class="headerlink" title="问题(46) 多态的实现"></a>问题(46) 多态的实现</h1><h1 id="问题-47-静态绑定和动态绑定的介绍"><a href="#问题-47-静态绑定和动态绑定的介绍" class="headerlink" title="问题(47) 静态绑定和动态绑定的介绍"></a>问题(47) 静态绑定和动态绑定的介绍</h1><h1 id="问题-49-结构体内存对⻬⽅式和为什么要进⾏内存对⻬？"><a href="#问题-49-结构体内存对⻬⽅式和为什么要进⾏内存对⻬？" class="headerlink" title="问题(49) 结构体内存对⻬⽅式和为什么要进⾏内存对⻬？"></a>问题(49) 结构体内存对⻬⽅式和为什么要进⾏内存对⻬？</h1><h1 id="问题-50-内存泄漏的定义，如何检测与避免？"><a href="#问题-50-内存泄漏的定义，如何检测与避免？" class="headerlink" title="问题(50) 内存泄漏的定义，如何检测与避免？"></a>问题(50) 内存泄漏的定义，如何检测与避免？</h1><h1 id="问题-51-说⼀下平衡⼆叉树、⾼度平衡⼆叉树（AVL）"><a href="#问题-51-说⼀下平衡⼆叉树、⾼度平衡⼆叉树（AVL）" class="headerlink" title="问题(51) 说⼀下平衡⼆叉树、⾼度平衡⼆叉树（AVL）"></a>问题(51) 说⼀下平衡⼆叉树、⾼度平衡⼆叉树（AVL）</h1><h1 id="问题-52-说⼀下红⿊树（RB-tree）"><a href="#问题-52-说⼀下红⿊树（RB-tree）" class="headerlink" title="问题(52) 说⼀下红⿊树（RB-tree）"></a>问题(52) 说⼀下红⿊树（RB-tree）</h1><h1 id="问题-53-说⼀下-fork，wait，exec-函数"><a href="#问题-53-说⼀下-fork，wait，exec-函数" class="headerlink" title="问题(53) 说⼀下 fork，wait，exec 函数"></a>问题(53) 说⼀下 fork，wait，exec 函数</h1><h1 id="问题-54-构造函数的执⾏算法？"><a href="#问题-54-构造函数的执⾏算法？" class="headerlink" title="问题(54) 构造函数的执⾏算法？"></a>问题(54) 构造函数的执⾏算法？</h1><h1 id="问题-55-⼿写实现智能指针类"><a href="#问题-55-⼿写实现智能指针类" class="headerlink" title="问题(55) ⼿写实现智能指针类"></a>问题(55) ⼿写实现智能指针类</h1><h1 id="问题-56-C-11特性"><a href="#问题-56-C-11特性" class="headerlink" title="问题(56) C++11特性"></a>问题(56) C++11特性</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;问题-1-c-中内存的分布情况&quot;&gt;&lt;a href=&quot;#问题-1-c-中内存的分布情况&quot; class=&quot;headerlink&quot; title=&quot;问题(1) c++中内存的分布情况&quot;&gt;&lt;/a&gt;问题(1) c++中内存的分布情况&lt;/h1&gt;&lt;h1 id=&quot;问题-2-栈和堆的</summary>
      
    
    
    
    
    <category term="工作" scheme="https://frankho-hwc.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
    <category term="C++" scheme="https://frankho-hwc.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>GAN讲解</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/GAN%E8%AE%B2%E8%A7%A3/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/GAN%E8%AE%B2%E8%A7%A3/</id>
    <published>2023-03-09T08:49:11.000Z</published>
    <updated>2023-03-09T08:49:11.230Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>resnet讲解</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/</id>
    <published>2023-03-09T08:47:51.000Z</published>
    <updated>2023-03-12T02:40:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a>论文链接</h1><p>论文地址 <a href="https://arxiv.org/abs/1512.03385">[1512.03385] Deep Residual Learning for Image Recognition (arxiv.org)</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="深度神经网络退化问题"><a href="#深度神经网络退化问题" class="headerlink" title="深度神经网络退化问题"></a>深度神经网络退化问题</h2><p>从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果，从下图也可以看出网络深度越深拟合程度就越好。但是随着深度越深，网络的性能就一定会更好吗？</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-606573bdaaa97de6b8b10fb00f76d29a_720w.png" alt="img"></p><p>实验发现，随着网络深度增加时，网络准确度出现饱和，甚至出现下降。这就是退化问题(degradation problem)。由下图可知，56层的网络比20层网络效果还要差。这不会是过拟合问题，因为56层网络的训练误差同样高。虽然我们知道深度神经网络存在梯度消失或者梯度爆炸的问题，但是存在的一些技术并没有解决这些问题。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-dcf5688dad675cbe8fb8be243af5e1fd_720w.png" alt="20层与56层网络在CIFAR-10上的误差"></p><h1 id="残差学习"><a href="#残差学习" class="headerlink" title="残差学习"></a>残差学习</h1><p><strong>残差</strong>即观测值与估计值之间的差</p><p>假设我们要建立深层网络，当我们不断堆积新的层，但增加的层什么也不学习（极端情况），那么我们就仅仅复制浅层网络的特征，即新层是浅层的恒等映射（Identity mapping），这样深层网络的性能应该至少和浅层网络一样，那么退化问题就得到了解决。</p><p>对于一个堆积层结构（几层堆积而成）当输入为$x$ 时其学习到的特征记为 $H(x)$ ，现在我们希望其可以学习到残差 $F(x) = H(x) - x$ ，这样其实原始的学习特征是$F(x)+x $ 。之所以这样是因为残差学习相比原始特征直接学习更容易。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构如图4所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。如图所示。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.png" alt="img"></p><p>从直观上看，残差学习需要学习的内容较少，从数学角度来说，如下所示</p><p>首先残差单元可以表示为</p><script type="math/tex; mode=display">y_l = h(x_i) + F(x_i,W_l) \\x_{l + 1} = f(y_l)</script><p>其中$x<em>l$和$x</em>{l+1}$ 分别表示的是第$l$个残差单元的输入和输出，注意每个残差单元一般包含多层结构。$F$是残差函数，表示学习到的残差，而 $h(x_l) = x_l$表示恒等映射， $f$是ReLU激活函数。基于上式，我们求得从浅层$l$到深层 $L$的学习特征为</p><script type="math/tex; mode=display">x_{L} = X_l +     \sum_{i = l}^{L-1}F(x_i,W_i)</script><p>利用链式规则，可以求得反向过程的梯度</p><script type="math/tex; mode=display">\frac{\delta loss}{\delta x_l} = \frac{\delta loss}{\delta x_L} \cdot \frac{\delta x_L}{\delta x_l} =  \frac{\delta loss}{\delta x_L} \cdot (1 + \frac{\delta}{\delta x_l}\sum_{i = l}^{L-1}F(x_i,W_i))</script><p>式子的第一个因子$\frac{\delta loss}{\delta x_l}$ 表示的损失函数到达$L$ 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。要注意上面的推导并不是严格的证明。</p><h1 id="ResNet网络结构"><a href="#ResNet网络结构" class="headerlink" title="ResNet网络结构"></a>ResNet网络结构</h1><p>ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元。如下图所示，变化主要体现在ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。ResNet的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。从下图中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。下图展示的34-layer的ResNet，还可以构建更深的网络如表1所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-7cb9c03871ab1faa7ca23199ac403bd9_720w.webp" alt="resnet structure"></p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-1dfd4022d4be28392ff44c49d6b4ed94_720w.webp" alt="不同的resnet结构"></p><p>再来说一下resnet的block结构，ResNet block有两种，一种两层结构，一种是三层的bottleneck结构，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1，它通过1x1 conv来巧妙地缩减或扩张feature map维度，从而使得我们的3x3 conv的filters数目不受上一层输入的影响，它的输出也不会影响到下一层。中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原。既保持了模型精度又减少了网络参数和计算量，节省了计算时间。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述"></p><p>注意：<br>对于短路连接，如果残差映射F(x)的维度与跳跃连接x的维度不同，那咱们是没有办法对它们两个进行相加操作的，必须对x进行升维操作，让他俩的维度相同时才能计算：</p><p>zero-padding全0填充增加维度：<br>此时一般要先做一个downsamp，可以采用stride=2的pooling，这样不会增加参数<br>采用新的映射（projection shortcut）：<br>一般采用1x1的卷积，这样会增加参数，也会增加计算量。</p><h2 id="改进版的ResNet"><a href="#改进版的ResNet" class="headerlink" title="改进版的ResNet"></a>改进版的ResNet</h2><p>改进前后一个明显的变化是采用pre-activation，BN和ReLU都提前了。而且作者推荐短路连接采用恒等变换，这样保证短路连接不会有阻碍。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/cc9ccbd34fe3cf9b201fe520523a71f3.jpeg" alt="在这里插入图片描述"></p><h1 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h1><p>对比18-layer和34-layer的网络效果，如图所示。可以看到普通的网络出现退化现象，但是ResNet很好的解决了退化问题。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-ac88d9e118e3a85922188daba84f7efd_720w.webp" alt="img"></p><p>最后展示一下ResNet网络与其他网络在ImageNet上的对比结果，如表所示。可以看到ResNet-152其误差降到了4.49%，当采用集成模型后，误差可以降到3.57%。</p><p><img src="/2023/03/09/resnet%E8%AE%B2%E8%A7%A3/v2-0a2c8a209a221817f91c1f1728327beb_720w.webp" alt="img"></p><h1 id="Resnet的pytorch实现"><a href="#Resnet的pytorch实现" class="headerlink" title="Resnet的pytorch实现"></a>Resnet的pytorch实现</h1><p>此处实现的是resnet50结构</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment">#每个stage维度中扩展的倍数</span></span><br><span class="line">    extention=<span class="number">4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,inplanes,planes,stride,downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param inplanes: 输入block的之前的通道数</span></span><br><span class="line"><span class="string">        :param planes: 在block中间处理的时候的通道数</span></span><br><span class="line"><span class="string">                planes*self.extention:输出的维度</span></span><br><span class="line"><span class="string">        :param stride:</span></span><br><span class="line"><span class="string">        :param downsample:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1=nn.Conv2d(inplanes,planes,kernel_size=<span class="number">1</span>,stride=stride,bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1=nn.BatchNorm2d(planes)</span><br><span class="line"></span><br><span class="line">        self.conv2=nn.Conv2d(planes,planes,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2=nn.BatchNorm2d(planes)</span><br><span class="line"></span><br><span class="line">        self.conv3=nn.Conv2d(planes,planes*self.extention,kernel_size=<span class="number">1</span>,stride=<span class="number">1</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3=nn.BatchNorm2d(planes*self.extention)</span><br><span class="line"></span><br><span class="line">        self.relu=nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#判断残差有没有卷积</span></span><br><span class="line">        self.downsample=downsample</span><br><span class="line">        self.stride=stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#参差数据</span></span><br><span class="line">        residual=x</span><br><span class="line"></span><br><span class="line">        <span class="comment">#卷积操作</span></span><br><span class="line">        out=self.conv1(x)</span><br><span class="line">        out=self.bn1(out)</span><br><span class="line">        out=self.relu(out)</span><br><span class="line"></span><br><span class="line">        out=self.conv2(out)</span><br><span class="line">        out=self.bn2(out)</span><br><span class="line">        out=self.relu(out)</span><br><span class="line"></span><br><span class="line">        out=self.conv3(out)</span><br><span class="line">        out=self.bn3(out)</span><br><span class="line">        out=self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#是否直连（如果Indentity blobk就是直连；如果Conv2 Block就需要对残差边就行卷积，改变通道数和size</span></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual=self.downsample(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#将残差部分和卷积部分相加</span></span><br><span class="line">        out+=residual</span><br><span class="line">        out=self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block,layers,num_class</span>):</span><br><span class="line">        <span class="comment">#inplane=当前的fm的通道数</span></span><br><span class="line">        self.inplane=<span class="number">64</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#参数</span></span><br><span class="line">        self.block=block</span><br><span class="line">        self.layers=layers</span><br><span class="line"></span><br><span class="line">        <span class="comment">#stem的网络层</span></span><br><span class="line">        self.conv1=nn.Conv2d(<span class="number">3</span>,self.inplane,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1=nn.BatchNorm2d(self.inplane)</span><br><span class="line">        self.relu=nn.ReLU()</span><br><span class="line">        self.maxpool=nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#64,128,256,512指的是扩大4倍之前的维度，即Identity Block中间的维度</span></span><br><span class="line">        self.stage1=self.make_layer(self.block,<span class="number">64</span>,layers[<span class="number">0</span>],stride=<span class="number">1</span>)</span><br><span class="line">        self.stage2=self.make_layer(self.block,<span class="number">128</span>,layers[<span class="number">1</span>],stride=<span class="number">2</span>)</span><br><span class="line">        self.stage3=self.make_layer(self.block,<span class="number">256</span>,layers[<span class="number">2</span>],stride=<span class="number">2</span>)</span><br><span class="line">        self.stage4=self.make_layer(self.block,<span class="number">512</span>,layers[<span class="number">3</span>],stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#后续的网络</span></span><br><span class="line">        self.avgpool=nn.AvgPool2d(<span class="number">7</span>)</span><br><span class="line">        self.fc=nn.Linear(<span class="number">512</span>*block.extention,num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#stem部分：conv+bn+maxpool</span></span><br><span class="line">        out=self.conv1(x)</span><br><span class="line">        out=self.bn1(out)</span><br><span class="line">        out=self.relu(out)</span><br><span class="line">        out=self.maxpool(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#block部分</span></span><br><span class="line">        out=self.stage1(out)</span><br><span class="line">        out=self.stage2(out)</span><br><span class="line">        out=self.stage3(out)</span><br><span class="line">        out=self.stage4(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#分类</span></span><br><span class="line">        out=self.avgpool(out)</span><br><span class="line">        out=torch.flatten(out,<span class="number">1</span>)</span><br><span class="line">        out=self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_layer</span>(<span class="params">self,block,plane,block_num,stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param block: block模板</span></span><br><span class="line"><span class="string">        :param plane: 每个模块中间运算的维度，一般等于输出维度/4</span></span><br><span class="line"><span class="string">        :param block_num: 重复次数</span></span><br><span class="line"><span class="string">        :param stride: 步长</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        block_list=[]</span><br><span class="line">        <span class="comment">#先计算要不要加downsample</span></span><br><span class="line">        downsample=<span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span>(stride!=<span class="number">1</span> <span class="keyword">or</span> self.inplane!=plane*block.extention):</span><br><span class="line">            downsample=nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplane,plane*block.extention,stride=stride,kernel_size=<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(plane*block.extention)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Conv Block输入和输出的维度（通道数和size）是不一样的，所以不能连续串联，他的作用是改变网络的维度</span></span><br><span class="line">        <span class="comment"># Identity Block 输入维度和输出（通道数和size）相同，可以直接串联，用于加深网络</span></span><br><span class="line">        <span class="comment">#Conv_block</span></span><br><span class="line">        conv_block=block(self.inplane,plane,stride=stride,downsample=downsample)</span><br><span class="line">        block_list.append(conv_block)</span><br><span class="line">        self.inplane=plane*block.extention</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Identity Block</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,block_num):</span><br><span class="line">            block_list.append(block(self.inplane,plane,stride=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*block_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">resnet=ResNet(Bottleneck,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>],<span class="number">1000</span>)</span><br><span class="line">x=torch.randn(<span class="number">64</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">X=resnet(x)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/31852747">你必须要知道CNN模型：ResNet - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/m0_54487331/article/details/112758795">https://blog.csdn.net/m0_54487331/article/details/112758795</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;论文链接&quot;&gt;&lt;a href=&quot;#论文链接&quot; class=&quot;headerlink&quot; title=&quot;论文链接&quot;&gt;&lt;/a&gt;论文链接&lt;/h1&gt;&lt;p&gt;论文地址 &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;[1512.03385] D</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>Gram矩阵</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/Gram%E7%9F%A9%E9%98%B5/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/Gram%E7%9F%A9%E9%98%B5/</id>
    <published>2023-03-09T08:47:04.000Z</published>
    <updated>2023-03-09T14:04:56.893Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前置知识-向量的内积"><a href="#前置知识-向量的内积" class="headerlink" title="前置知识-向量的内积"></a>前置知识-向量的内积</h1><p>等之后补充，或者自行线性代数学习。</p><h1 id="什么是Gram矩阵"><a href="#什么是Gram矩阵" class="headerlink" title="什么是Gram矩阵"></a>什么是Gram矩阵</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)，很明显，这是一个对称矩阵。</p><p><img src="/2023/03/09/Gram%E7%9F%A9%E9%98%B5/v2-d9cbea9872a6accbc98df52e5d3b1599_720w.webp" alt="gram matrix"></p><p><img src="/2023/03/09/Gram%E7%9F%A9%E9%98%B5/v2-c31c8591818dcb5ea833b9a1b1253f29_720w.webp" alt="img"></p><p>也可以说，格拉姆矩阵$G = A^TA$，其中$A$为某个向量矩阵</p><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>下面的都是列向量Gram矩阵</p><ol><li><p>Gram矩阵是对称矩阵</p><script type="math/tex; mode=display">G^T = (A^TA)^T = A^TA = G</script></li><li><p>对于实矩阵</p><script type="math/tex; mode=display">r(A^TA) = r(A)</script></li><li><p>若$A^TA = 0$,则$A = 0$</p></li><li><p>对于实矩阵 $A$, 则 $A^TA$是半正定矩阵</p><script type="math/tex; mode=display">x^TA^TAx = (A^Tx)^TAX \geq 0</script></li><li><p>对于任意$n$阶实对称半正定矩阵$M$, 存在矩阵$A$使得$M =A^TA$成立.</p><p>因为矩阵$M$实对称, 所以$M$可以正交对角化, 即</p><script type="math/tex; mode=display">M = Q^T\Lambda Q</script></li></ol><p>   又因为矩阵$M$半正定, 所以其特征值$\lambda_i \geq 0$, 所以可记</p><script type="math/tex; mode=display">   \Lambda^{\frac{1}{2}} = diag(\sqrt{\lambda_1},...,\sqrt{\lambda_i})</script><p>   且$A =\Lambda^{\frac{1}{2}} Q^T$</p><p>   则有</p><script type="math/tex; mode=display">   M = Q\Lambda Q^T      =(\Lambda^{\frac{1}{2}}Q^T)^T\Lambda^{\frac{1}{2}}Q^T     =A^TA</script><ol><li><p>若$A = [\alpha_1 … \alpha_n]$列满秩, 则$A^TA$正定</p><p>由性质2知，$r(A^TA)=r(A) = n$</p><p>因为$Ax = 0$只有零解, 结合性质 (4), 对于非零$x \in \mathbb{R}^n$</p><script type="math/tex; mode=display">x^TA^TAx = (Ax)^TAx > 0</script></li></ol><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>在深度学习中，可以用于Style Transference。</p><p>这个以后再填坑</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/105470826">Gram 矩阵 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/187345192">格拉姆矩阵（Gram matrix）详细解读 - 知乎 (zhihu.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前置知识-向量的内积&quot;&gt;&lt;a href=&quot;#前置知识-向量的内积&quot; class=&quot;headerlink&quot; title=&quot;前置知识-向量的内积&quot;&gt;&lt;/a&gt;前置知识-向量的内积&lt;/h1&gt;&lt;p&gt;等之后补充，或者自行线性代数学习。&lt;/p&gt;
&lt;h1 id=&quot;什么是Gram矩</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性代数" scheme="https://frankho-hwc.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>transformer讲解</title>
    <link href="https://frankho-hwc.github.io/2023/03/09/transformer%E8%AE%B2%E8%A7%A3/"/>
    <id>https://frankho-hwc.github.io/2023/03/09/transformer%E8%AE%B2%E8%A7%A3/</id>
    <published>2023-03-09T08:46:48.000Z</published>
    <updated>2023-03-09T08:46:48.027Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Normalization串讲</title>
    <link href="https://frankho-hwc.github.io/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/"/>
    <id>https://frankho-hwc.github.io/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/</id>
    <published>2023-03-08T13:23:40.000Z</published>
    <updated>2023-03-09T11:26:27.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Normalization的目的就是使数据分布服从均值为0，方差为1的标准正态分布(高斯分布).其目的在于使神经元输入的数据是独立同分布的，这样可以使得网络更快收敛，而且又不会出现梯度消失的问题。</p><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h2><p>在深度学习中，网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：<strong>“Internal Covariate Shift”</strong>。</p><p>出现ICS问题，就会导致每个神经元的输入数据不再是“独立同分布”了，则会导致</p><ol><li>上层参数需要不断适应新的输入数据分布，降低学习速度。</li><li>下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</li><li>每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li></ol><p>所以为了解决这个问题，就提出了批量归一化</p><p>具体做法如下</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/webp.webp" alt="img"></p><p>为什么最后规范化之后还要尺度变换和平移呢，这个操作是一个归一化的反操作，给予两个参数$\gamma$和$\beta$让神经网络自己去学习，让它自己琢磨normalization到底有没有起到优化作用。</p><h2 id="Batch-Normalization-后的效果"><a href="#Batch-Normalization-后的效果" class="headerlink" title="Batch Normalization 后的效果"></a>Batch Normalization 后的效果</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-95f654fdf99999db3fa7dab0bbfbc358_720w.webp" alt="img"></p><p>输入数据被归一到高斯分布区间，激活函数的敏感性更强了</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-b31f7d863179f5f0b93d40c4fabbc31a_720w.webp" alt="img"></p><p>经过激活函数后的数据更加平滑，有利于之后的训练</p><h2 id="BatchNormalizaiton-图解"><a href="#BatchNormalizaiton-图解" class="headerlink" title="BatchNormalizaiton 图解"></a>BatchNormalizaiton 图解</h2><p><strong>1维</strong></p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/62ae8f9dadb346308041b7ad909be734-16783450068039.png" alt="img"></p><p><strong>2维</strong></p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/183a59ee09834831851930e0a580abcd.png" alt="img"></p><p><strong>3维</strong></p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/0bff3dea63de40549f1e5abbc2f8cc8f.png" alt="img"></p><h2 id="加入BN后的效果"><a href="#加入BN后的效果" class="headerlink" title="加入BN后的效果"></a>加入BN后的效果</h2><ol><li>加快网络收敛速度</li><li>缓解梯度爆炸和防止梯度消失，由上图<a href="##Batch Normalization 后的效果">Batch Normalization后的效果</a></li><li>防止过拟合</li></ol><h1 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h1><h2 id="详解-1"><a href="#详解-1" class="headerlink" title="详解"></a>详解</h2><p>如果是遇到样本序列长度不同的时候，如RNN，transformer等，无法使用BN来进行归一化，如下图这种情况，如果使用BN的话，会出现batchsize过小的问题。同时样本数过少时，BN也不能发挥出它应该有的效果。</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/3ca9141cc2224748993520f6db520bf4.png" alt="img"></p><p>具体公式如下</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/image-20230309192458979.png" alt="image-20230309192458979"></p><p>LN中同层神经元的输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。</p><p>对于特征图</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/7000.png" alt="img"></p><p> ，LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。其均值和标准差公式为：</p><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/7000-16783611681623.png" alt="img"></p><h2 id="与BN的对比"><a href="#与BN的对比" class="headerlink" title="与BN的对比"></a>与BN的对比</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-c039daa05cd9d5c3936c4513422690b0_720w.jpeg" alt="img"></p><p>如图所示，左边是LayerNormalizaiton，而右边是BatichNormalization。BN是按照batch来切的，batch中每一个样本的同一个维度来进行normalization。而LN则是对同一个样本的不同通道进行normalization。</p><h2 id="LN的效果"><a href="#LN的效果" class="headerlink" title="LN的效果"></a>LN的效果</h2><p>同样地，LN也能够很好地缓解ICS问题。</p><h1 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h1><h2 id="详解-2"><a href="#详解-2" class="headerlink" title="详解"></a>详解</h2><h2 id="BN-LN-IN三者对比"><a href="#BN-LN-IN三者对比" class="headerlink" title="BN,LN,IN三者对比"></a>BN,LN,IN三者对比</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-94c40b6f6f41e45f5d254906d70c10ee_720w.webp" alt="img"></p><p>如上图所示，最左边是LN,中间是BN，最右边是IN。</p><h1 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h1><h2 id="详解-3"><a href="#详解-3" class="headerlink" title="详解"></a>详解</h2><p>GN介于LN和IN之间，其首先将channel分为许多组（group），对每一组做归一化，及先将feature的维度由[N, C, H, W]reshape为[N, G，C//G , H, W]，归一化的维度为[C//G , H, W]</p><h2 id="BN-LN-IN-GN四者对比"><a href="#BN-LN-IN-GN四者对比" class="headerlink" title="BN,LN,IN,GN四者对比"></a>BN,LN,IN,GN四者对比</h2><p><img src="/2023/03/08/Normalization%E4%B8%B2%E8%AE%B2/v2-fad3333df9a87c1c4f1db4b20557da6f_720w.webp" alt="img"></p><p>BatchNorm：batch方向做归一化，算N<em>H</em>W的均值<br>LayerNorm：channel方向做归一化，算C<em>H</em>W的均值<br>InstanceNorm：一个channel内做归一化，算H<em>W的均值<br>GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)</em>H*W的均值</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/480425962">Internal Covariate Shift问题 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/24810318">什么是批标准化 (Batch Normalization) - 知乎 (zhihu.com)</a></p><p><a href="https://www.jianshu.com/p/a78470f521dd">内部协变量偏移(Internal Covariate Shift)和批归一化(Batch Normalization) - 简书 (jianshu.com)</a></p><p><a href="https://blog.csdn.net/Mike_honor/article/details/125915321">(38条消息) 标准化（Normalization）知识点总结_normalization操作汇总_CV技术指南的博客-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/54530247">模型优化之Layer Normalization - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/492803886">Transformer中的归一化(五)：Layer Norm的原理和实现 &amp; 为什么Transformer要用LayerNorm - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/56542480">模型优化之Instance Normalization - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/35005794">全面解读Group Normalization-（吴育昕-何恺明 ） - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/duanshao/article/details/80055887">(38条消息) 组归一化（Group Normalization）的解释_技术修行的博客-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/74476637">深度学习之17——归一化(BN+LN+IN+GN) - 知乎 (zhihu.com)</a></p><p><a href="https://cloud.tencent.com/developer/article/1526775">https://cloud.tencent.com/developer/article/1526775</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;Normalization的目的就是使数据分布服从均值为0，方差为1的标准正态分布(高斯分布).其目的在于使神经元输入的数据是独立同分布的，</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>双线性插值卷积与反卷积</title>
    <link href="https://frankho-hwc.github.io/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/"/>
    <id>https://frankho-hwc.github.io/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/</id>
    <published>2023-03-08T08:06:31.000Z</published>
    <updated>2023-03-08T13:36:22.371Z</updated>
    
    <content type="html"><![CDATA[<h1 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h1><p>在介绍反卷积之前，先介绍一下卷积</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>计算机视觉里的卷积操作本质上就是数学分析里的卷积详情见下面链接</p><p>具体操作为</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308191953527.png" alt="image-20230308191953527"></p><p>用矩阵法来理解为</p><p>假设输入图像为 4*4，元素矩阵为</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192113451.png" alt="image-20230308192113451"></p><p>卷积核大小为3*3</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192131123.png" alt="image-20230308192131123"></p><p>步长stride = 1， 填充padding = 0，按照卷积计算公式$output = \frac{i + 2p -k}{s} + 1$,则输出矩阵为2*2</p><p>将输入图像flatten成1维向量</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192245137.png" alt="image-20230308192245137"></p><p>同样地，将输出Flatten成1维向量</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192325607.png" alt="image-20230308192325607"></p><p>用矩阵运算来描述</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192336855.png" alt="image-20230308192336855"></p><p>推导可知稀疏矩阵$C$：</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308192351512.png" alt="image-20230308192351512"></p><h2 id="反卷积-1"><a href="#反卷积-1" class="headerlink" title="反卷积"></a>反卷积</h2><p>反卷积，顾名思义就是卷积的反操作，但是对于同一个卷积核（因非其稀疏矩阵不是正交矩阵），<strong>结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状</strong></p><p>以矩阵运算来描述</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-556e4849c9bf764fdbfb5ce84f0a204d_720w.webp" alt="img"></p><h2 id="棋盘效应"><a href="#棋盘效应" class="headerlink" title="棋盘效应"></a>棋盘效应</h2><p>在使用转置卷积时观察到一个棘手的现象（尤其是深色部分常出现）就是”<a href="https://www.zhihu.com/search?q=棋盘格子状伪影&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A1682194600}">棋盘格子状伪影</a>“，被命名为棋盘效应（Checkboard artifacts）。</p><p>棋盘效应是由于转置卷积的“不均匀重叠”（Uneven overlap）的结果。使图像中某个部位的颜色比其他部位更深。尤其是当卷积核（Kernel）的大小不能被步长（Stride）整除时，<a href="https://www.zhihu.com/search?q=反卷积&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A1682194600}">反卷积</a>就会不均匀重叠。虽然原则上网络可以通过训练调整权重来避免这种情况，但在实践中神经网络很难完全避免这种不均匀重叠。</p><p>​    在（a）中，步长为1，卷积核为$2*2$。如红色部分所展示，输入第一个像素映射到输出上第一个和第二个像素。而正如绿色部分，输入的第二个像素映射到输出上的第二个和第三个像素。则输出上的第二个像素从输入上的第一个和第二个像素接收信息。总而言之，输出中间部分的像素从输入中接收的信息存在重叠区域。在示例（b）中的卷积核大小增加到3时，输出所接收到的大多数信息的中心部分将收缩。但这并不是最大的问题，因为重叠仍然是均匀的。</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-9a93cbac03830084b90574f017b6038b_720w.webp" alt="img"></p><p>如果将步幅改为2，在卷积核大小为2的示例中，输出上的所有像素从输入中接收相同数量的信息。由下图（a）可见，此时描以转置卷积的重叠。若将卷积核大小改为4（下图（b）），则均匀重叠区域将收缩，与此同时因为重叠是均匀的，故仍然为有效输出。但如果将卷积核大小改为3，步长为2（下图（c）），以及将卷积核大小改为5，步长为2（下图（d）），问题就出现了，对于这两种情况输出上的每个像素接收的信息量与相邻像素不同。在输出上找不到连续且均匀重叠区域。</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-6288d1734ad00c718fc814e4c7bbc985_720w.webp" alt="img"></p><p> 在二维情况下棋盘效应更为严重，下图直观地展示了在二维空间内的棋盘效应。</p><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/v2-de1bb8f86193666e3b0a1539d273ab32_720w.webp" alt="img"></p><h3 id="如何避免"><a href="#如何避免" class="headerlink" title="如何避免"></a>如何避免</h3><p><strong>采取可以被步长整除的卷积核长度</strong><br>该方法较好地应对了棋盘效应问题，但仍然不够圆满，因为一旦我们的卷积核学习不均匀。</p><p><strong>线性插值</strong></p><h2 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h2><p>在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算<strong>(e.g.:图像的语义分割(segmentation))</strong>，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做<strong>上采样(Upsample)</strong></p><h2 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h2><p>下采样实际上就是缩小图像，主要目的是为了使得图像符合显示区域的大小，生成对应图像的缩略图。比如说在CNN中的池化层或卷积层就是下采样。不过卷积过程导致的图像变小是为了提取特征，而池化下采样是为了降低特征的维度。下采样层有两个作用：<br>一是减少计算量，防止过拟合；<br>二是增大感受野，使得后面的卷积核能够学到更加全局的信息。</p><h1 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h1><h2 id="双线性插值-Bilinear-Interpolation）"><a href="#双线性插值-Bilinear-Interpolation）" class="headerlink" title="双线性插值(Bilinear Interpolation）"></a>双线性插值(Bilinear Interpolation）</h2><p>在讲双线性插值卷积前，先讲单线性插值卷积</p><h3 id="单线性插值"><a href="#单线性插值" class="headerlink" title="单线性插值"></a>单线性插值</h3><p><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308163812548.png" alt="image-20230308163812548"></p><p>如图所示，已知中P1点和P2点，坐标分别为(x1, y1)、(x2, y2)，要计算 [x1, x2] 区间内某一位置 x 在直线上的y值</p><p>由两点确定一条直线可知</p><script type="math/tex; mode=display">\frac{y-y_1}{x - x_1} = \frac{y_2 - y_1}{x_2 - x_1}</script><p>经过整理可得</p><script type="math/tex; mode=display">y = \frac{x_2 - x}{x_2 - x_1}y_1 - \frac{x - x_1}{x_2 - x_1} y_2</script><p>首先看分子，分子可以看成x与x1和x2的距离作为权重，这也是很好理解的，P点与P1、P2点符合线性变化关系，所以P离P1近就更接近P1，反之则更接近P2。</p><p>现在再把公式中的分式看成一个整体，原式可以理解成y1与y2是加权系数，如何理解这个加权，要返回来思考一下，咱们先要明确一下根本的目的：咱们现在不是在求一个公式，而是在图像中根据2个点的像素值求未知点的像素值。这样一个公式是不满足咱们写代码的要求的。<br>现在根据实际的目的理解，就很好理解这个加权了，y1与y2分别代表原图像中的像素值，上面的公式可以写成如下形式：</p><script type="math/tex; mode=display">f(P) =\frac{x_2 - x}{x_2 - x_1}f(P_1) - \frac{x - x_1}{x_2 - x_1} f(P_2)</script><p>其中，$P$,$P_1$,$P_2$分别代表了被插值的像素和插值两边的像素</p><h3 id="再看双线性插值"><a href="#再看双线性插值" class="headerlink" title="再看双线性插值"></a>再看双线性插值</h3><p>双线性插值是分别在两个方向计算了共3次单线性插值，如图所示，先在x方向求2次单线性插值，获得R1(x, y1)、R2(x, y2)两个临时点，再在y方向计算1次单线性插值得出P(x, y)（实际上调换2次轴的方向先y后x也是一样的结果）。<br><img src="/2023/03/08/%E5%8F%8C%E7%BA%BF%E6%80%A7%E6%8F%92%E5%80%BC%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%8F%8D%E5%8D%B7%E7%A7%AF/image-20230308164511503.png" alt="image-20230308164511503"></p><p>首先是插值出$R_1$和$R_2$</p><script type="math/tex; mode=display">f(R_1) =\frac{x_2 - x}{x_2 - x_1}f(Q_{11}) - \frac{x - x_1}{x_2 - x_1} f(Q_{21}) \\f(R_2) =\frac{x_2 - x}{x_2 - x_1}f(Q_{12}) - \frac{x - x_1}{x_2 - x_1} f(Q_{22})</script><p>然后再通过$R_1$和$R_2$插值出P</p><script type="math/tex; mode=display">f(P) =\frac{y_2 - y}{y_2 - y_1}f(R_{1}) - \frac{y - y_1}{y_2 - y_1} f(R_{2})</script><p>总结起来就是</p><script type="math/tex; mode=display">f(x,y) = \frac{f(Q_{11})}{(x_2-x_1)(y_2-y_1)}(x_2-x)(y_2-y) + \frac{f(Q_{21})}{(x_2-x_1)(y_2-y_1)}(x - x_1)(y_2 - y) + \frac{f(Q_{12})}{(x_2-x_1)(y_2-y_1)}(x_2 - x_1)(y_2 -y _1) + \frac{f(Q_{22})}{(x_2-x_1)(y_2-y_1)}(x - x1)(y - y2)</script><h1 id="双线性插值与反卷积的关系"><a href="#双线性插值与反卷积的关系" class="headerlink" title="双线性插值与反卷积的关系"></a>双线性插值与反卷积的关系</h1><p>双线性插值与反卷积都可以用来实现上采样，而双线性插值可以通过反卷积实现。而插值方法不止一种，还有三线性插值等。插值方法的优点是可以避免棋盘效应。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/48501100">反卷积(Transposed Convolution)详细推导 - 知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/22298352">知乎 (zhihu.com)</a></p><p><a href="https://www.zhihu.com/question/48279880/answer/1682194600">https://www.zhihu.com/question/48279880/answer/1682194600</a></p><p>对上述帖子致以诚挚感谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;反卷积&quot;&gt;&lt;a href=&quot;#反卷积&quot; class=&quot;headerlink&quot; title=&quot;反卷积&quot;&gt;&lt;/a&gt;反卷积&lt;/h1&gt;&lt;p&gt;在介绍反卷积之前，先介绍一下卷积&lt;/p&gt;
&lt;h2 id=&quot;卷积&quot;&gt;&lt;a href=&quot;#卷积&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>名词解释-ablation</title>
    <link href="https://frankho-hwc.github.io/2023/03/08/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-Ablation%20study/"/>
    <id>https://frankho-hwc.github.io/2023/03/08/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-Ablation%20study/</id>
    <published>2023-03-08T07:56:36.000Z</published>
    <updated>2023-03-08T08:53:04.267Z</updated>
    
    <content type="html"><![CDATA[<h1 id="何为Ablation-study"><a href="#何为Ablation-study" class="headerlink" title="何为Ablation study"></a>何为Ablation study</h1><p><strong>Ablation study</strong>，意为消融实验，<strong>通常是指删除模型或算法的某些“功能”，并查看其如何影响性能。</strong></p><p>在论文中一般来说会提出多个创新方法，或者新型结构模块，或注意力模块等。这些东西在一起为模型的性能作出了贡献。然而为了了解每个部分单独能发挥的作用，常常会在论文中提出消融研究。</p><p>例如某论文提出了方法A,B,C。而该论文是基于某个baseline的改进。因此，在消融研究部分，会进行以下实验，baseline ，baseline+A，baseline+B, baseline+C, baseline+A+B+C等实验的各个评价指标有多少，从而得出每个部分所能发挥的作用有多大。</p><p>参考文章<a href="https://zhuanlan.zhihu.com/p/389091953">名词解释 | 论文中的Ablation study - 知乎 (zhihu.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;何为Ablation-study&quot;&gt;&lt;a href=&quot;#何为Ablation-study&quot; class=&quot;headerlink&quot; title=&quot;何为Ablation study&quot;&gt;&lt;/a&gt;何为Ablation study&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Ablati</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>EnLightenGAN</title>
    <link href="https://frankho-hwc.github.io/2023/03/04/EnLightenGAN/"/>
    <id>https://frankho-hwc.github.io/2023/03/04/EnLightenGAN/</id>
    <published>2023-03-04T12:03:15.000Z</published>
    <updated>2023-03-08T13:38:46.753Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EnligtenGAN"><a href="#EnligtenGAN" class="headerlink" title="EnligtenGAN"></a>EnligtenGAN</h1><h1 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h1><p>首先我们可以看到有scripts文件夹，我们运行的程序就在这个里面‘</p><p>通过分析代码我们可以知道实际运行的还是train.py和predict.py</p><p><img src="/2023/03/04/EnLightenGAN/image-20230306215659710.png" alt="image-20230306215659710"></p><p>然后我们可以发现有一个option文件夹，里面就是有附带的一些选项。</p><p>configs里有一个yaml文件，里面配置了enlightenGAN的超参数。</p><p>data文件夹里定义了dataloader，即如何将数据读取进去。</p><p>最关键的是models里的singlemodel和networks这两个py文件，其他文件我觉得应该是一些替代组件，应该是用来做消融实验的？</p><p>此处具体讲解的就是这两个文件</p><p>这个模型使用是一个U-Net架构的Generator和两个Discirminator,具体之后更新</p><h1 id="代码运行及一些问题"><a href="#代码运行及一些问题" class="headerlink" title="代码运行及一些问题"></a>代码运行及一些问题</h1><h2 id="运行步骤"><a href="#运行步骤" class="headerlink" title="运行步骤"></a>运行步骤</h2><h3 id="1-从github上git-clone源码"><a href="#1-从github上git-clone源码" class="headerlink" title="1. 从github上git clone源码"></a>1. 从github上git clone源码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/VITA-Group/EnlightenGAN</span><br></pre></td></tr></table></figure><h3 id="2-创建虚拟环境-也可以使用以前的环境-并安装依赖"><a href="#2-创建虚拟环境-也可以使用以前的环境-并安装依赖" class="headerlink" title="2.创建虚拟环境(也可以使用以前的环境),并安装依赖"></a>2.创建虚拟环境(也可以使用以前的环境),并安装依赖</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate enlighten</span><br><span class="line">pip install -r requirement.txt</span><br></pre></td></tr></table></figure><h3 id="3-创建文件夹-并将vgg预训练模型放入其中"><a href="#3-创建文件夹-并将vgg预训练模型放入其中" class="headerlink" title="3.创建文件夹,并将vgg预训练模型放入其中"></a>3.创建文件夹,并将vgg预训练模型放入其中</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir model</span><br></pre></td></tr></table></figure><p>模型地址：<a href="https://drive.google.com/file/d/1IfCeihmPqGWJ0KHmH-mTMi_pn3z3Zo-P/view?usp=sharing">https://drive.google.com/file/d/1IfCeihmPqGWJ0KHmH-mTMi_pn3z3Zo-P/view?usp=sharing</a></p><p>本人提供一个百度网盘 链接：<a href="https://pan.baidu.com/s/1qX97-7H3HCwllLwiBUo_Zg?pwd=ktnj">https://pan.baidu.com/s/1qX97-7H3HCwllLwiBUo_Zg?pwd=ktnj</a>  code：ktnj </p><h3 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h3><h4 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><p> …/final_dataset/trainA and …/final_dataset/trainB（即final_dataset文件夹与项目文件夹同级的位置），将<a href="https://drive.google.com/drive/folders/1fwqz8-RnTfxgIIkebFG2Ej3jQFsYECh0?usp=sharing">图片</a>下载分别放入</p><p>如无法下载，此处有百度网盘</p><p>链接：<a href="https://pan.baidu.com/s/1MpSKs5HVs6alMjfzQtm3rA?pwd=vgiy">https://pan.baidu.com/s/1MpSKs5HVs6alMjfzQtm3rA?pwd=vgiy</a>  code：vgiy </p><h4 id="可视化-可选"><a href="#可视化-可选" class="headerlink" title="可视化(可选)"></a>可视化(可选)</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python -m visdom.server -port=8097</span><br></pre></td></tr></table></figure><p>访问步骤</p><p>打开浏览器，输入<a href="http://localhost:8097/（可以实时观看图片结果）">http://localhost:8097/（可以实时观看图片结果）</a><br>如果在另一台电脑上跑，输入地址：地址号：端口号<br>如：10.162.34.109:8097</p><h4 id="进行训练"><a href="#进行训练" class="headerlink" title="进行训练"></a>进行训练</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python scripts/script.py --train</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>可能遇到问题</strong></p><p>如果你的集群的卡不是三张，请打开scripts文件夹下的script.py,将第37行 —gpu_ids 修改为你所拥有显卡数量的编号</p><p><img src="/2023/03/04/EnLightenGAN/image-20230304202357977.png" alt="image-20230304202357977"></p><p>我使用的是有两张3090的服务器，所以修改位0,1</p><p>否则将会出现这样的错误</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CustomDatasetDataLoader</span><br><span class="line">dataset [UnalignedDataset] was created</span><br><span class="line">#training images = 1016</span><br><span class="line">single</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;train.py&quot;, line 19, in &lt;module&gt;</span><br><span class="line">    model = create_model(opt)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/models.py&quot;, line 36, in create_model</span><br><span class="line">    model.initialize(opt)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/single_model.py&quot;, line 40, in initialize</span><br><span class="line">    self.vgg = networks.load_vgg16(&quot;./model&quot;, self.gpu_ids)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/networks.py&quot;, line 1051, in load_vgg16</span><br><span class="line">    vgg = torch.nn.DataParallel(vgg, gpu_ids)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 142, in __init__</span><br><span class="line">    _check_balance(self.device_ids)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 23, in _check_balance</span><br><span class="line">    dev_props = _get_devices_properties(device_ids)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 458, in _get_devices_properties</span><br><span class="line">    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 458, in &lt;listcomp&gt;</span><br><span class="line">    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 441, in _get_device_attr</span><br><span class="line">    return get_member(torch.cuda)</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/_utils.py&quot;, line 458, in &lt;lambda&gt;</span><br><span class="line">    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]</span><br><span class="line">  File &quot;/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 299, in get_device_properties</span><br><span class="line">    raise AssertionError(&quot;Invalid device id&quot;)</span><br><span class="line">AssertionError: Invalid device id</span><br></pre></td></tr></table></figure><p>此外，还要修改一个地方</p><p><img src="/2023/03/04/EnLightenGAN/image-20230304203105671.png" alt="image-20230304203105671"></p><p>否则会报错，是因为YAML 5.1版本后弃用了yaml.load(file)这个用法，因为觉得很不安全，5.1版本之后就修改了需要指定Loader，通过默认加载器（FullLoader）禁止执行任意函数，该load函数也变得更加安全。</p><p>此外还有个报错</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model [SingleGANModel] was created</span><br><span class="line">Setting up a new session...</span><br><span class="line">create web directory ./checkpoints/enlightening/web...</span><br><span class="line">/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.</span><br><span class="line">  warnings.warn(&quot;nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.&quot;)</span><br><span class="line">/home/ubuntu/anaconda3/envs/IAT/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.</span><br><span class="line">  &quot;See the documentation of nn.Upsample for details.&quot;.format(mode))</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;train.py&quot;, line 37, in &lt;module&gt;</span><br><span class="line">    errors = model.get_current_errors(epoch)</span><br><span class="line">  File &quot;/home/ubuntu/hwc/EnlightenGAN-master/models/single_model.py&quot;, line 413, in get_current_errors</span><br><span class="line">    D_A = self.loss_D_A.data[0]</span><br><span class="line">IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item&lt;T&gt;()` in C++ to convert a 0-dim tensor to a number</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这个就是把single_model.py里所有的data[0]改成.item就行了</p><h3 id="5-评估"><a href="#5-评估" class="headerlink" title="5.评估"></a>5.评估</h3><h4 id="仅测试时："><a href="#仅测试时：" class="headerlink" title="仅测试时："></a>仅测试时：</h4><p>下载pretrained model，放入./checkpoints/enlightening中</p><h4 id="创建文件夹-1"><a href="#创建文件夹-1" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><p>…/test_dataset/testA and …/test_dataset/testB（即test_dataset文件夹与项目文件夹同级的位置），将自己要测试的图片放入testA，在testB中至少存入一张随机图片</p><h4 id="运行测试代码"><a href="#运行测试代码" class="headerlink" title="运行测试代码"></a>运行测试代码</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python scripts/script.py --predict</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;EnligtenGAN&quot;&gt;&lt;a href=&quot;#EnligtenGAN&quot; class=&quot;headerlink&quot; title=&quot;EnligtenGAN&quot;&gt;&lt;/a&gt;EnligtenGAN&lt;/h1&gt;&lt;h1 id=&quot;代码讲解&quot;&gt;&lt;a href=&quot;#代码讲解&quot; class=&quot;</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://frankho-hwc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="低亮图像增强" scheme="https://frankho-hwc.github.io/tags/%E4%BD%8E%E4%BA%AE%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/"/>
    
    <category term="计算机视觉" scheme="https://frankho-hwc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>JavaIO机制</title>
    <link href="https://frankho-hwc.github.io/2023/03/02/JavaIO%E6%9C%BA%E5%88%B6/"/>
    <id>https://frankho-hwc.github.io/2023/03/02/JavaIO%E6%9C%BA%E5%88%B6/</id>
    <published>2023-03-02T01:08:36.000Z</published>
    <updated>2023-03-02T02:22:19.778Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、Java-I-O流定义"><a href="#一、Java-I-O流定义" class="headerlink" title="一、Java I/O流定义"></a>一、Java I/O流定义</h1><p>在Java中，流是从源读取并写入目标的数据序列。</p><p><strong>输入流</strong>是从源读取数据，<strong>输出流</strong>是将数据写入目标</p><p>下面就具体讲解Java I/O流</p><h1 id="二、Java-字节流"><a href="#二、Java-字节流" class="headerlink" title="二、Java 字节流"></a>二、Java 字节流</h1><h2 id="2-1-输入流"><a href="#2-1-输入流" class="headerlink" title="2.1 输入流"></a>2.1 输入流</h2><h3 id="2-1-1-方法"><a href="#2-1-1-方法" class="headerlink" title="2.1.1 方法"></a>2.1.1 方法</h3><p>InputStream读入的单位是字节，具体方法如下</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">read() <span class="comment">// 从输入流中读取数据的下一个字节</span></span><br><span class="line">read(<span class="type">byte</span>[]b) <span class="comment">//从输入流中读取一定长度的字节，并以整数的形式返回字节数</span></span><br><span class="line">mark（<span class="type">int</span> readlimit) <span class="comment">// 在输入流当前位置放一个标记，readlimit参数告知此输入流在标记位置失效之前允许读取的字节数</span></span><br><span class="line">reset()  <span class="comment">//将输入指针返回标记处</span></span><br><span class="line">skip(<span class="type">long</span> n) <span class="comment">//跳过n个单位的字节，并返回实际跳过的字节数</span></span><br><span class="line">markSupported() <span class="comment">//是否支持mark</span></span><br><span class="line">close()<span class="comment">//关闭流</span></span><br></pre></td></tr></table></figure><p>但是并不是所有InputStream类都支持上述方法。</p><h3 id="2-1-2-种类"><a href="#2-1-2-种类" class="headerlink" title="2.1.2 种类"></a>2.1.2 种类</h3><p>以下为常见的三个输入流</p><h4 id="FileInputStream"><a href="#FileInputStream" class="headerlink" title="FileInputStream"></a>FileInputStream</h4><p>该流是用于读取文件中信息的输入流</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String args[])</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">byte</span>[] array = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">100</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">InputStream</span> <span class="variable">input</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="string">&quot;input.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">&quot;文件中的可用字节: &quot;</span> + input.available());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//从输入流中读取字节</span></span><br><span class="line">            input.read(array);</span><br><span class="line">            System.out.println(<span class="string">&quot;从文件读取的数据: &quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将字节数组转换为字符串</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(array);</span><br><span class="line">            System.out.println(data);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭输入流</span></span><br><span class="line">            input.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.getStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>示例</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这是文件中的一行文本。</span><br></pre></td></tr></table></figure><p>结果</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">文件中的可用字节: 35</span><br><span class="line">从文件读取的数据:</span><br><span class="line">这是文件中的一行文本。</span><br></pre></td></tr></table></figure><h4 id="InputStream"><a href="#InputStream" class="headerlink" title="InputStream"></a>InputStream</h4><p>为了使用InputStream,首先需要引入java.io.InputStream包</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">InputStream</span> <span class="variable">object1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(); <span class="comment">// 创建输入流对象</span></span><br></pre></td></tr></table></figure><p>示例</p><h4 id="BufferedInputStream"><a href="#BufferedInputStream" class="headerlink" title="BufferedInputStream"></a>BufferedInputStream</h4><p>BufferedInputStream 是为I/O流增加缓冲区，提高性能,该输入流有两个注入方法</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BufferedInputStream(InputStream in) <span class="comment">//默认size为32</span></span><br><span class="line">BufferedInputStream(InputStream in, <span class="type">int</span> size)</span><br></pre></td></tr></table></figure><h2 id="2-2-输出流"><a href="#2-2-输出流" class="headerlink" title="2.2 输出流"></a>2.2 输出流</h2><h3 id="2-2-1-方法"><a href="#2-2-1-方法" class="headerlink" title="2.2.1 方法"></a>2.2.1 方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">write(<span class="type">int</span> b) <span class="comment">//将指定的字节写入输入流</span></span><br><span class="line">write(<span class="type">byte</span>[]b) <span class="comment">//将b个字节从指定的byte数组写入此输入流</span></span><br><span class="line">write(<span class="type">byte</span>[]b,<span class="type">int</span> off, <span class="type">int</span> len) <span class="comment">//将指定byte数组中从偏移量off开始的len个字节写入此输出流</span></span><br><span class="line">flush() <span class="comment">//彻底完成输出并清空缓存区</span></span><br><span class="line">close() <span class="comment">//关闭输出流</span></span><br></pre></td></tr></table></figure><h3 id="2-2-2-种类"><a href="#2-2-2-种类" class="headerlink" title="2.2.2 种类"></a>2.2.2 种类</h3><h4 id="FileOutputStream"><a href="#FileOutputStream" class="headerlink" title="FileOutputStream"></a>FileOutputStream</h4><p>创建方式</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//包括布尔型参数</span></span><br><span class="line"><span class="type">FileOutputStream</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(String path, <span class="type">boolean</span> value);</span><br><span class="line"></span><br><span class="line"><span class="comment">//不包括布尔型参数</span></span><br><span class="line"><span class="type">FileOutputStream</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(String path);</span><br></pre></td></tr></table></figure><p>写入文件</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> <span class="string">&quot;这是文件中的一行文本。&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">FileOutputStream</span> <span class="variable">output</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="string">&quot;output.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">byte</span>[] array = data.getBytes();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将字节写入文件</span></span><br><span class="line">            output.write(array);</span><br><span class="line"></span><br><span class="line">            output.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">            e.getStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果</p><p><img src="/2023/03/02/JavaIO%E6%9C%BA%E5%88%B6/image-20230302101400910.png" alt="写入文件"></p><h4 id="OutputStream"><a href="#OutputStream" class="headerlink" title="OutputStream"></a>OutputStream</h4><p>与InputStream对应</p><h4 id="BufferedOutputStream"><a href="#BufferedOutputStream" class="headerlink" title="BufferedOutputStream"></a>BufferedOutputStream</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BufferedOutputStream(OutputStream in) <span class="comment">//默认size为32</span></span><br><span class="line">BufferedOutputStream(OutputStream in, <span class="type">int</span> size)</span><br></pre></td></tr></table></figure><h2 id="三、-Java的字符流"><a href="#三、-Java的字符流" class="headerlink" title="三、 Java的字符流"></a>三、 Java的字符流</h2><h3 id="3-1-Reader"><a href="#3-1-Reader" class="headerlink" title="3.1 Reader"></a>3.1 Reader</h3><p>Reader是用于读取字符的字符流，Stream可能会出现读取汉字乱码的现象</p><h3 id="3-1-1分类"><a href="#3-1-1分类" class="headerlink" title="3.1.1分类"></a>3.1.1分类</h3><h4 id="BufferedReader"><a href="#BufferedReader" class="headerlink" title="BufferedReader"></a>BufferedReader</h4><h4 id="FileReader"><a href="#FileReader" class="headerlink" title="FileReader"></a>FileReader</h4><h3 id="3-2Writrer"><a href="#3-2Writrer" class="headerlink" title="3.2Writrer"></a>3.2Writrer</h3><h2 id="3-2-1"><a href="#3-2-1" class="headerlink" title="3.2.1"></a>3.2.1</h2><h4 id="BufferedWriter"><a href="#BufferedWriter" class="headerlink" title="BufferedWriter"></a>BufferedWriter</h4><h4 id="FlieReader"><a href="#FlieReader" class="headerlink" title="FlieReader"></a>FlieReader</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、Java-I-O流定义&quot;&gt;&lt;a href=&quot;#一、Java-I-O流定义&quot; class=&quot;headerlink&quot; title=&quot;一、Java I/O流定义&quot;&gt;&lt;/a&gt;一、Java I/O流定义&lt;/h1&gt;&lt;p&gt;在Java中，流是从源读取并写入目标的数据序列。&lt;/</summary>
      
    
    
    
    
    <category term="java学习" scheme="https://frankho-hwc.github.io/tags/java%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>JVM学习(1)</title>
    <link href="https://frankho-hwc.github.io/2023/02/27/JVM%E5%AD%A6%E4%B9%A0-(1)/"/>
    <id>https://frankho-hwc.github.io/2023/02/27/JVM%E5%AD%A6%E4%B9%A0-(1)/</id>
    <published>2023-02-27T14:12:03.000Z</published>
    <updated>2023-03-02T01:32:38.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是JVM"><a href="#什么是JVM" class="headerlink" title="什么是JVM"></a>什么是JVM</h1><p>JVM(Java Virtual Machine)即java的虚拟机，是用于在实际的计算机上仿真模拟计算机的实现</p><p><img src="/2023/02/27/JVM%E5%AD%A6%E4%B9%A0-(1)/d947f91e44c44c6c80222b49c2dee859-new-image19a36451-d673-486e-9c8e-3c7d8ab66929.png" alt="java虚拟机示意图"></p><p>​    如图所示，这个就是JVM结构图</p><h1 id="Java内存区域"><a href="#Java内存区域" class="headerlink" title="Java内存区域"></a>Java内存区域</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是JVM&quot;&gt;&lt;a href=&quot;#什么是JVM&quot; class=&quot;headerlink&quot; title=&quot;什么是JVM&quot;&gt;&lt;/a&gt;什么是JVM&lt;/h1&gt;&lt;p&gt;JVM(Java Virtual Machine)即java的虚拟机，是用于在实际的计算机上仿真模拟计算机的</summary>
      
    
    
    
    
    <category term="java学习" scheme="https://frankho-hwc.github.io/tags/java%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="工作" scheme="https://frankho-hwc.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>杂言</title>
    <link href="https://frankho-hwc.github.io/2023/02/26/%E6%9D%82%E8%A8%80/"/>
    <id>https://frankho-hwc.github.io/2023/02/26/%E6%9D%82%E8%A8%80/</id>
    <published>2023-02-26T12:44:35.000Z</published>
    <updated>2023-02-27T11:50:57.672Z</updated>
    
    <content type="html"><![CDATA[<p>2022年12月25日，青岛开发区四中，一年一度的全国研究生考试开始了。青岛刺骨的寒风，没有温度的太阳。一场猝不及防的新冠，以及对数学的恐惧，不出意外，考砸了，没书读了。但是个人还是无法接受这个结果，虽然明知是自己自作自受:前三年如果博一把，就不用遭这门罪;如果再对自己狠点，就可以发挥好些了……</p><p>考研这半年来，啥罪都遭了，啥好事都没轮到自己。看着保研的天天笑嘻嘻，看着留学的炫自己的offer，现在出成绩了，又要看着上岸的同辈的通知书了。看着自己的同辈就要进入下一个人生阶段了，再看看自己原地踏步，想想真是五味杂陈。没办法啊，谁叫你技不如人呢？谁叫你明事理太晚了呢？谁叫你脸皮薄不去找老师进实验室呢？明明有高人指点，居然还能落后别人。应该反省反省自己。</p><p>那就反省反省吧。我觉得我这个人，1、做事太拖沓了；2、心态还是出了大问题，还是不能太急躁了；3、碰到事情还是太直来直去了，不知变通；4、抗压能力还是差了很多；5、缺少定力，容易对外界事物影响；6、有时候还是管不住自己，承接上点；7、某些时候又有点自负了。总之个人的缺点还是很多，没上岸也说明老天可能觉得我必须要改完这些才有资格上岸吧。</p><p>那既然没上岸，首要议程就是接下来怎么办，因为要毕业了嘛。首先想到的自然是找工作。但是好巧不巧，今年的形势出奇的差:首先是今年互联网大寒冬，各大公司大裁员；其次是今年hc大幅下降，招收的人数少了很多；然后就是本人准备考研去了，对找工作的技能项目方面没有做好任何准备，这个才是最关键的。那就考公吧，家里人的意思就是这样的，我寻思了一下，好像也只能这样，然后我就等着我阳康之后就开始准备了。我个人其实是不想考公务员的，但是现实让我不得不这么做。我准备了一个来月，行测可以做到70多一些，申论就是听天由命了。在复习过程中，我还在反复地精神内耗，只能说很痛苦。2月25日，我参加了我人生中第一次公务员考试，也是我在高考后第一次写一篇考试作文。行测可以说差强人意，申论那更是勉勉强强了。考完了就先放一边，等出成绩再做打算。然而这些都只是权宜之计，真正的目的还是为了服务二战。</p><p>本人考研考的是某华5，11408。这次考的非常低，可能国家线都没有，比别人裸考都要低，真是搞笑。这次我还是决定要再冲一次，无论如何。本人的目标很单纯，就是想看看博士是什么样，给这些东西祛魅，然后做一点微不足道的贡献吧。其次就是想润出去看看，本人观念跟父母冲突很大，不想屈从于父母意志，也不想被宣传机器洗脑，还有就是对现实的不满。这些就是我现在的动力。但是，这次考的很差，而且对手都很强，加之经济形势不好，越来越多的人加入考研。二战压力很大，这也是我精神内耗的原因之一。二战的事情之后再说，还有10个月做准备。本人已经复习过一轮了，第二轮只能拿出老命了。</p><p>此外，呆在家里压力很大。俗话说，距离产生美。一回家，自己妈妈的态度就一百八十度大转弯。这时候我才看明白，原来我妈是不想让我考研的，虽然她表面上是支持我考，暗地里只是想让我考个公呆家里一辈子。但是我爹还是很赞成我二战的，但是还是要我先考公再说，毕竟还是有个工作稳妥些。说是这么说，真呆在家里只会是压力拉满，什么娱乐放松都不能有(虽然今年多半是不会有什么了)，还有天天言语上的压力，再加上社会自己同辈，考上只会是难上加难。这也是我精神内耗的来源之一。</p><p>但是，现在还是要有实际行动，虽然天天看牛客陌陌压力很大，可还是要准备工作啊，不能因为啥都没学就不去尝试。所以我打算先花点时间去学习一点java类后端的知识，先看情况找个实习做做，这样一能积累经验，二万一二战没上也能找工作，毕竟凡事都是说不准的。寒假自己拿着模板写了一份简历，但是发现自己没什么可写的，之前也没有专门为找工作而准备过，所以看看能不能上实习。</p><p>到现在总结一下，目前就是分三个方向，考公，就业，二战。前两者都是为第三者准备，第三者才是目前我认为的重中之重。其实现实的困难算是其次，最主要的困难是自己精神上的内耗：怕应届生这个好身份找不到什么好工作，怕自己没能力找工作，怕自己二战上不了岸，又看到别人光鲜亮丽，反正就是瞻前顾后，无所适从。天天刷知乎，但是知乎上的消息源又是压力来源之一。加上自己旁边一起考研的发挥很好，基本上岸，那就显得更慌了。这段时间算是我从小到大最失败的一段时间，也是最难过的一段时间。到现在，我明白了，要想过去，就只能熬，啥都不想，停下内耗，孤注一掷，反正已经是最差了也没什么可怕了。毕竟有句话说得好，不想赢就不会输。只有往前看才行，不管现在形势如何，硬着头皮也得上。</p><p>至于具体如何行动，可以分为三个部分。考公不必说，这个等出成绩通知面试再谈。找工作先去想办法润色一下简历，然后背一背八股学一些技术，找要求低一些，薪资过得去的公司面试，或者就直接实习。考研嘛，上面说了，压力拉满，所以最好是出去考研。其实还有个机会，就是出国，但是家里拿不出这么多钱，而且我没有选择刷绩点，所以也没戏。其次最主要的就是保持好心态，改掉自己的坏习惯，反思一下自己为什么会没考上，然后戒掉无用社交，一心一意拿下考研。还有就是多跟有经验的人交流，参考参考别人给的意见。</p><p>总之接下来会是一段非常难过的日子，希望我能够挨过去，拿到自己想要的东西。如果有人无意看到了这个，请吸取教训，要么直接工作，要么直接梭哈，不看别人任何消息，每年都会有重复这样悲剧的，希望不是看到这个的人。也希望自己一年后是笑着看到这个东西的。</p><p>随便写写，逻辑有点混乱，就这样凑合吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2022年12月25日，青岛开发区四中，一年一度的全国研究生考试开始了。青岛刺骨的寒风，没有温度的太阳。一场猝不及防的新冠，以及对数学的恐惧，不出意外，考砸了，没书读了。但是个人还是无法接受这个结果，虽然明知是自己自作自受:前三年如果博一把，就不用遭这门罪;如果再对自己狠点</summary>
      
    
    
    
    <category term="杂言" scheme="https://frankho-hwc.github.io/categories/%E6%9D%82%E8%A8%80/"/>
    
    
    <category term="工作" scheme="https://frankho-hwc.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
    <category term="考研" scheme="https://frankho-hwc.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
    <category term="碎碎念" scheme="https://frankho-hwc.github.io/tags/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"/>
    
    <category term="人生" scheme="https://frankho-hwc.github.io/tags/%E4%BA%BA%E7%94%9F/"/>
    
  </entry>
  
  <entry>
    <title>这是一个实验文章</title>
    <link href="https://frankho-hwc.github.io/2023/02/13/%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AE%9E%E9%AA%8C%E6%96%87%E7%AB%A0/"/>
    <id>https://frankho-hwc.github.io/2023/02/13/%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E5%AE%9E%E9%AA%8C%E6%96%87%E7%AB%A0/</id>
    <published>2023-02-13T03:20:49.000Z</published>
    <updated>2023-02-13T03:42:59.862Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个实验文章</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是一个实验文章&lt;/p&gt;
</summary>
      
    
    
    
    <category term="exp" scheme="https://frankho-hwc.github.io/categories/exp/"/>
    
    
    <category term="实验" scheme="https://frankho-hwc.github.io/tags/%E5%AE%9E%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://frankho-hwc.github.io/2023/02/11/hello-world/"/>
    <id>https://frankho-hwc.github.io/2023/02/11/hello-world/</id>
    <published>2023-02-11T07:14:32.696Z</published>
    <updated>2023-02-11T07:12:45.531Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
